[["index.html", "Introduction to the principles of data wrangling and visualisation in R Preface", " Introduction to the principles of data wrangling and visualisation in R Dan Olner 2021-08-13 Preface "],["introduction.html", "1 Introduction", " 1 Introduction The basics this document covers are: Using RStudio and RStudio projects Starting to script Using libraries Vectors and why theyre boring-but-important Using dataframes and loading simple data If you have any problems or questions, feel free to get in touch with me: d.olner@sheffield.ac.uk - though please do try google as well, if you can! And look out for simple typos in your code, if its not working. Experienced programmers suffer from this as much as anyone else - be vigilant, double-check all the dots, brackets etc. Completing the whole doc from scratch will probably take about three hours at a gentle pace. Everything you need is in the folder you unzipped, including the PDF youre reading. Well be using that folder again below so dont delete it. Right, lets jump right in. "],["getting-r-and-rstudio.html", "2 Getting R and RStudio", " 2 Getting R and RStudio In order to learn the basics, youll need a computer with R and RStudio installed. If you need to install these, its very easy. Either use the following links or use google to find them: Install RStudio with the download appropriate to your operating system at the bottom of this page: https://www.rstudio.com/products/rstudio/download/ Before running RStudio, also install R itself from here, again choosing the link for your operating system: https://cran.rstudio.com/ Both of these will provide you with self-installers, there should be nothing awkward to do. Once installed, run RStudio and lets begin "],["r-studio.html", "3 R-Studio 3.1 A first look at RStudio 3.2 Entering commands in R 3.3 Assigning to variables 3.4 Opening a project in RStudio 3.5 Creating a new script and running code in it 3.6 But will it make sense in two months time? Using comments and sections 3.7 Its all about the libraries 3.8 Loading a file into a dataframe and examining the data", " 3 R-Studio 3.1 A first look at RStudio RStudio is a self-contained environment for doing everything in R. As well see throughout the day, it does a heap of different things to make programming in R as painless as possible. First thing - open RStudio and have a look at whats there. You should see something like the following. Yours may have a different colour scheme, dont worry about that: RStudio presents you with these panes: The console pane (left in the above image - note it says console at the top of the window) where we can enter commands and run them immediately. The environment pane (top right): all our data and variables will appear here. A pane with a number of tabs (bottom right), currently open on files. When we first produce graphics, these will appear here in the plots pane. 3.2 Entering commands in R Before we do anything else, lets get a feel for how to enter commands in R. Well use the console to do this. If the cursor isnt already blinking in the console, click anywhere in the console pane. The console will execute anything we put here as soon as you press enter. Try some of these or something different just to get a feel for it, pressing enter after each. Any maths will be evaluated and returned. 80 80+20 80*20 80/20 &#39;This is some text&#39; sqrt(100) That last one - sqrt(100) - is a function. Functions are in/out machines: in this case, stick 100 in and get its square root out. Anything you put inside brackets in R is going into a function. You can stick anything in there that will evaluate, so we could also have done: sqrt(45+55) ## [1] 10 sqrt(sqrt(16)) ## [1] 2 Take note of that simple point about functions when we put a function within another one: it means we can combine functions and outputs in whatever way we want. This makes R very powerful and creative. 3.3 Assigning to variables Everything, whether its a single number, a list of numbers, a piece of text, an entire dataset or a graph, can be assigned to a variable. Variable names should be a balance between brevity and clarity: you want it to say something that will make sense to you when returning to the code a month later, but it also needs to be typeable. (Although as well see, RStudio removes a lot of the pain of typing for us with its autocomplete feature, so if you do want very descriptive variable names, go for it.) Heres some examples to try in the console. Rs assignment operator is a less than followed by a minus: Typing both of these the whole time can be a pain - so RStudio has a handy shortcut key: ALT + minus Have a go at using this shortcut key when assigning these examples: city &lt;- &#39;London&#39; max &lt;- 150 min &lt;- 45 range &lt;- max - min Youll see when assigning to variable names in the console, it doesnt automatically output whats just been assigned. You will, however, see them appear in the environment pane on the right: your new variables are there, under values. You can also output these assigned values by typing the variable names into the console, as we were doing with simple values before: city ## [1] &quot;London&quot; min ## [1] 45 max ## [1] 150 range ## [1] 105 And of course we can then use the variable names in functions: sqrt(range) ## [1] 10.24695 Well get on to putting all this into a reusable script in a moment. 3.4 Opening a project in RStudio RStudio projects are self-contained folders that keep everything for a particular project together in one place. They also provide a number of really useful features to help make life easier, as well see. The folder you unpacked to get at this PDF is also an RStudio project. Lets open it in RStudio now. Access RStudios project loading dialogue in the top right. It should currently just say project (none). Click to get a range of project options. You can tell RStudio that an existing folder should become a project - but were opening one thats already there, so choose open project: Navigate to the folder you unzipped, open the folder and double-click the UBDC_R_viz_intro_primer.Rproj file. You may not immediately see much change - but now the whole workspace will be saved, including any variables you can see in the environment panel. As well see below, now were in an RStudio project, we dont have to worry about where the working directory is: RStudio sets it automatically to our project folder. As long as were working in that folder, theres no need to mess around with the full path to the file. (It also allows us to move RStudio projects and give them easily to other people to use.) Note that in the top right you can now see the project name. The `files tab in the bottom right can also be used to access files in the folder. 3.5 Creating a new script and running code in it Youll program all of the work in this document into a single script file in RStudio. To get a blank script to start with, go to: File / new file / new R-script Note how the menus tell you what shortcut key to use for a lot of actions. In this case we can see Ctrl+Shift+N will also create a new script. The script opens in a new tab in a pane above the console. Currently its just named Untitled1 (on the tab itself, at the top). Click in the script pane to move the cursor there. Start scripting! Just add a couple of lines, whatever you like - use some of the commands we entered into the console above, for example. Note: unlike the console these lines wont run until we tell them to run. So just add whatever you want to add over a few lines, pressing enter to move to the next one. Well run the lines in a moment. Once youve added something, the name (currently Untitled1 still) will turn red: you can now save it. Save the new script either via the menu (File/save) or CTRL + S. Give it whatever name you like. Note that it will save in the top level of your project folder. RStudio will give it the extension .R Now we can run our first few commands. In a script, we have a few ways to choose, depending on what we want to do. A quick note about the programming philosophy of R. Where some programming languages are all about writing entire programs, compiling them and running them as a whole, start to finish, working with R is much more iterative and experimental. R can run entire programs - thats all the libraries are, after all - but its also designed for experimenting with and exploring data on the fly. Well be doing a lot of this in the workshop today. OK, so heres three options for running your script: To run a single line of code (as we were doing in the console): place your cursor anywhere on a single line you want to run. Then press CTRL + R or CTRL + ENTER. Both do the same, so whichever works for you. This will run that single line. Youll see it echoed in the console, as well as the output of the command, same as when we ran it directly in the console. There is also the option of using the run button at the top right of the script pane, but thats generally more faff than using the keyboard. To run multiple lines of code: highlight more than one line of script, as you would highlight text in any text editor or word processor. Then, as before, use the keyboard run commands, either CTRL + R or CTRL + ENTER. You can highlight the text with the mouse, or use the keyboard. If using the mouse, you can also use the mouse wheel to scroll the script while highlighting. If youre not familiar with keyboard shortcuts for highlighting: hold down shift then use the up and down arrows to highlight a row at a time. Third: you can run the entire script this way just by selecting everything with CTRL + A (or right-click and select all) Personally, I pretty much never do this, which is why Ive left it until last. When you save your RStudio workspace in the project, it will save all of the variables and progress youve made so far, so there is usually no reason to run an entire script from scratch every time you start work. (That doesnt mean some things shouldnt run in their entirety - but its good to put these things into functions in their own script. We wont be doing that, but its worth mentioning.) 3.6 But will it make sense in two months time? Using comments and sections Code you write today may not make the slightest bit of sense in the near future. This happens to all programmers. So be kind to your future self and let them know why you did what you did. Its absolutely essential to take some steps to make life easier for yourself by making sure the code is readable and clear. Leave plenty of space in your code wherever you can for a start. But the most essential way to make sure it makes sense: Comment all your code clearly Dont ever say to yourself, oh, this will make sense later. So comment all your code clearly! R uses the #hash symbol for comments. So for your few first script lines, you can add a comment or two thus (obviously, make your comments match what script you wrote!): #Oh look, this is a comment! It starts with a hash #Using the assignment operator city &lt;- &#39;London&#39; max &lt;- 150 min &lt;- 45 #Finding the range by substracting max from min range &lt;- max - min RStudio also provides a hugely useful comment feature: Adding four dashes to the end of a comment automatically makes it a section. Try it - add four dashes to your first comment. #Using the assignment operator---- As you add the dash, youll see a little down-pointing triangle appear on the left. Also, at the bottom of the scripting window, the same phrase should have appeared. Once we add more sections, you can click in that area to move between them. Shortcut key tip: ALT + SHIFT + J brings up the section menu without having to click on it. (At the moment weve only got one section, mind.) Some people also like to make their sections more visibly distinct by doing something like the comment below. Its a matter of personal taste - whatever helps you keep your code clear. #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #Using the assignment operator---- #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Well look at a few other keeping-things-readable ideas as we go on. 3.7 Its all about the libraries Commands already built into R (like the sqrt function we used above) are known as the base commands. But all the really awesome stuff in R comes from libraries. If theres something you think you need to do in R, someone has most probably already built a library to do it. For this workshop, well be (mostly) using a set of libraries developed by one person: Hadley Wickham. Hadleys designed these libraries with an underlying data philosophy - tidy data - so that as much as possible there is one clean, standardised way of doing things. Well use these libraries to get us from loading and organising data right through to visualising it in the ggplot library. If a library isnt already installed, you can install it with the following. (This is the first library well be using, in the next section.) Do this in the console, not in your script, as it only needs running once. RStudio will download and install the library. (So youll need to be connected to the internet.) install.packages(&#39;readr&#39;) Once the library is installed, you can load it ready for use with the following. This time, put this right at the top of your script, before all the code and comments youve just written as youll want to load these libraries every time you come back to the script. library(readr) Notice, just to be awkward, that when installing libraries, the name needs to be in quotes but when loading it, its not. As well see shortly, readr gives us a nicer way of loading CSVs than base R provides. ##A quick look at vectors (boring but essential for later) In R, a vector is just a collection of numbers, characters or other R object types. This may not seem terribly exciting right now, but understanding how R uses vectors helps massively with a lot of more complex activities well be looking at later. Equally, not understanding how R uses them can lead to all sorts of confusion. As we go along, well return to the concept of the vector and how R uses it. Its actually very simple but easy to misunderstand initially. Heres how they work. Where before we were assigning single numbers bobsAge &lt;- 45  a vector of numbers just looks like this. When scripting, you indicate its a vector with a c, enclose the values in round brackets and separate values with commas. In your own script, after your previous commands, make a vector similar to this with six random ages. everyonesAge &lt;- c(45,35,72,23,11,19) This should be familiar enough to anyone whos ever worked with mathematical vectors. And its the same structure for any other variable type, like strings. This time, make a vector of six names - pick whatever names you like but try and split them evenly between male and female. everyonesName &lt;- c(&#39;bob&#39;,&#39;gill&#39;,&#39;geoff&#39;,&#39;gertrude&#39;,&#39;cecil&#39;,&#39;zoe&#39;) You can either run each line after youve written it, or write both lines first, select both of them and run them together, as explained above. As before for single values, if you type these variable names into the console and press enter, youll see the whole vector. RStudio provides excellent code completion that massively helps with scripting. As an introduction to this: when you start to type either of the two variables we just made, you should see an autocomplete box like this: You can scroll through these with up and down arrows or use the mouse. Choose one now and press enter - it will be added to the code. You can also use CTRL + SPACE to bring up the autocomplete box at other times. everyonesAge ## [1] 45 35 72 23 11 19 everyonesName ## [1] &quot;bob&quot; &quot;gill&quot; &quot;geoff&quot; &quot;gertrude&quot; &quot;cecil&quot; &quot;zoe&quot; You can also access the individual values in a vector by using its index in square brackets after the variable, where 1 is the first entry and - in this case - 6 is the last: everyonesAge[1] ## [1] 45 everyonesAge[5] ## [1] 11 everyonesName[2] ## [1] &quot;gill&quot; everyonesName[3] ## [1] &quot;geoff&quot; R also has syntax for giving a range of integers. Type this directly into the console to get all numbers from 2 to 5: 2:5 ## [1] 2 3 4 5 So we can use this to access values in our vector. (Its up to you whether you want to run these directly in the console or put it in your script so you can save it.) everyonesName[2:5] ## [1] &quot;gill&quot; &quot;geoff&quot; &quot;gertrude&quot; &quot;cecil&quot; Now: an example that begins to show why vectors are so important in R. If we want to access different names in our vector of names, we do the following. Heres all the men and women separately (note, youll have to check your vectors to see what index the male/female members have - dont just copy these numbers.): everyonesName[c(1,3,5)] ## [1] &quot;bob&quot; &quot;geoff&quot; &quot;cecil&quot; everyonesName[c(2,4,6)] ## [1] &quot;gill&quot; &quot;gertrude&quot; &quot;zoe&quot; What happened there? We used another vector of numbers to index our earlier vector. R is built on vectors in this way. As weve already seen that vectors can be assigned to variables, we can also do the following: women &lt;- c(2,4,6) men &lt;- c(1,3,5) #Replace the vectors with their variable representation everyonesName[women] ## [1] &quot;gill&quot; &quot;gertrude&quot; &quot;zoe&quot; everyonesName[men] ## [1] &quot;bob&quot; &quot;geoff&quot; &quot;cecil&quot; Another way to access the contents of vectors is using Rs boolean values - that is, the values of TRUE and FALSE. If we use a vector of TRUE/FALSE values, for example, we can mark as TRUE if any of the names are female: everyonesName[c(FALSE,TRUE,FALSE,TRUE,FALSE,TRUE)] ## [1] &quot;gill&quot; &quot;gertrude&quot; &quot;zoe&quot; You can also use T and F as short-hand for TRUE and FALSE in R to save typing, if you want. For either, they must be in upper-case, however: everyonesName[c(F,T,F,T,F,T)] For any vector members where this is TRUE, it returns that value for us. Why should you care about this? Because if we can use TRUE and FALSE to access vector indices, we can ask questions of the vector using {conditionals}. For example, if we assume our everyonesAge vector is telling us the age of the people in everyonesName, we can ask, whos over 30 years old? (this is why we created six ages and six names above - make sure both of your age and name vectors have six entries each): everyonesName[everyonesAge &gt; 30] ## [1] &quot;bob&quot; &quot;gill&quot; &quot;geoff&quot; What just happened? Nothing more than we just did above when we used a vector of TRUE/FALSE values, except that R uses a conditional to create them. If you put this directly into the console - everyonesAge &gt; 30 ## [1] TRUE TRUE TRUE FALSE FALSE FALSE that just returns a vector containing TRUE/FALSE values from asking whether the ages in everyonesAge are more than 30. And as before, we can just stick that vector straight into everyonesName to find out whos over 30. This is the full list of logical operators that we can use to ask questions like this. Note especially the double equals: this will allow us to find exact matches for values and text. OK, so I totally lied about that being a quick look. But its all going to be very useful. The ability to use vectors in this way to access our data will turn out to be essential. But thats quite enough of vectors for now. They will crop up again shortly as we look at why theyre so important to how R thinks. But onto to the marginally more exciting 3.8 Loading a file into a dataframe and examining the data Well be keeping all of our data in dataframes. The easiest way to understand these is just to look at one. Your project folder has a sub-folder called data. All of the data youll use today is in there. Lets load an example dataframe to play with. Make this a new section in your script using comment code as we did above - something like the following (remembering to put in four dashes to turn it into a section): #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #Loading an example dataframe---- #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ While were here, time for another top tip. Youll often be creating code that you want to copy and move - well be doing this a lot shortly. RStudio has a couple of fantastic shortcut keys to help with this. If you want to practice these, use them for making the section heading / comment above. We can do this: Make the first comment line - a hash followed by a line of tildes, as above. Put the cursor on your new line. Now: SHIFT + ALT + DOWN ARROW. This makes a copy of the line below. Lastly, move your newly copied line one down to make room for the section heading itself: just use ALT + UP or DOWN arrows to move it. Then just add in the final header in the middle. This ability to quickly copy and move code around will come in very useful later. Its worth having these shortcuts at your fingertips. RStudio has the full list of shortcut keys in the help menu. Youll even see, pleasingly, theres a shortcut key to get the list of shortcut keys! Note all the cheatsheets in that menu too. Now, Load the example data from the CSV file from the data folder with the following, putting it into your script below the section heading. Take care - thats an underscore, not a full stop, between read and csv df &lt;- read_csv(&#39;data/averagePricePerTTWA.csv&#39;) The read_csv command (from the readr package you installed and loaded above) loads the CSV data, turns it into a dataframe and assigns it to our variable name, df. We can now use df to work with the data. Readr will also do its best to work out what type of data is in each column for you. So notice the message output that appears in the console when you use read_csv. ## Parsed with column specification: ## cols( ## ttwa_name = col_character(), ## avHousePrice = col_double(), ## numberOfSales = col_double() ## ) As with our last assignments, the dataframe also appears in the environment pane under data, telling us there are 166 observations and 3 variables. Heres another really useful RStudio autocomplete feature too: if youre trying to get a file thats in the project folder, once youve typed your quote marks, CTRL+SPACE will give you file autocomplete options. So for example, if you type the following and then, after the slash, press CTRL+SPACE, the full csv name will appear, as its the only file in the folder. If there are many files, RStudio provides a list. df &lt;- read_csv(&#39;data/&#39;) We can look directly at this data in a few ways. A really useful one is this: Click anywhere on the df variable name, or to the right of it, in its line in the environment pane. When youre in the right place, the cursor will turn into a hand. This opens the dataframe in its own tab. You can scroll through the data. This shows us the basic structure of the dataframe, familiar enough from programs like Excel: variable names are in columns and each row is an observation. So weve got ttwa_name: TTWA is short for Travel to Work Area. These have towns and cities at their centre and include the surrounding area from which most people commute. Theres also the average house price and a count of sales. So going back to that output to the console when we loaded the CSV: we can see it told us not only the column names but the variable type that read_csv assigned it. ttwa_name is a character column, avHousePrice is double (can have many decimal places) and numberOfSales are integers. If you have data with dates in, read_csv also does some other clever type assignment that makes dates easy to work with. Rs base CSV loader (read.csv uses a dot not an underscore) wont do this for us. Manually formatting dates is a pain to be avoided! We can also peek at the data using code. These two functions will show you the top and bottom few values if you input the dataframe into them. (This is the kind of quick looking at data that the console is good for.) #Look at the top and bottom of the dataframe head(df) tail(df) We have another great autocomplete feature for dataframes. In the console, type the name of the dataframe and then a dollar sign df$ You should see a list of the dataframes variable names appear, as below. You can access these as we did with the variable names above. Choose one now and press enter - it will be added to the code. Press enter again to run the line: R will output all 166 values for that variable to the console. How do we access particular observations or values in a dataframe? The syntax is similar to what we used with vectors: square brackets after the df variable name. Except its now two dimensions. Dataframes are referenced like this: df[row,column] So to access the price of the property in the first row: df[1,2] You can also access entire rows or columns. To look at the first row, for example: df[1,] We just leave the column index blank, meaning show all columns. The same applies for columns but with a slightly confusing difference, so to access whole columns, for now, just stick to using e.g. df$ttwa_name as we did above. We can use conditionals in our dataframe indices in exactly the same way we did with vectors. Say, for example, we want to keep only those cities that had over 100,000 sales. Just use the conditional in the row index (remembering the comma so we get all columns): highSalesCount &lt;- df[df$numberOfSales &gt; 100000,] Checking in the environment panel, This new dataframe has 63 cities/towns compared to 166 in the original df. All of these methods for examining whats in the dataframe can also be used for assigning TO the dataframe. If we want to add a new column, for example, its a simple matter: df$newcolumn &lt;- 1 Looking back at the dataframe view tab shows this new column is now present: Its not a very exciting column, admittedly: weve just added a 1 to all rows. But note Rs behaviour when we asked it to do this: we passed it a single number and R took this as a request to add it to every row in our new column. This can be used if we combine it with conditionals to ask questions about the data. We already saw how to ask a simple vector about something - we found people over the age of 30. Now we can do the same with a dataframe, because: Dataframe columns are just vectors. So everything we can do with everyonesName we can also do with df$avHousePrice and the other columns. Say we want to know which cities have an average house price above £150,000. First we can just add another single-value column, as we just did, and fill it with zeroes to start with: df$priceAbove150K &lt;- 0 Then we can overwrite rows above that value with a flag by selecting the elements of the vector with a conditional just as we did above with ages: df$priceAbove150K[df$avHousePrice &gt; 150000] &lt;- 1 Our new column now contains the original zeroes - and ones in those rows that met our price condition: Just the same as before, when we do this in the console, you can see it returns a vector of TRUE/FALSE values: df$avHousePrice &gt; 150000 But this time weve used our TRUE/FALSES to assign a value to those members of the vector Now weve done that, we can use the table function to give us a count of how many cities/towns are above and below our price difference: table(df$priceAbove150K) ## ## 0 1 ## 85 81 So 81 TTWAs have an average house price above £150,000. Its worth taking some time to check it makes sense: that the same principles for basic vectors apply to dataframe columns, which are themselves also just vectors. We can prove that just by showing how you build a dataframe from scratch rather than loading it. We could use our previous two vectors thus: people &lt;- data.frame(ages = everyonesAge, names = everyonesName) Youll see a people dataframe appear top-right in the environment pane, along with the dataframe we loaded. We can look at it as we did before: OK, thats all the basics we need to cover. Look forward to seeing you! "],["introduction-1.html", "4 Introduction", " 4 Introduction the principles of shaping data in preparation for visualising in ggplot. The first principle is: shaping the data is an essential part of building visualisations. Its not just an annoyance: your visualisation goals will dictate the kind of shape the data needs to be made into. Well use libraries from the tidyverse to do this. As Hadley Wickham says, by constraining your options, it simplifies how you can think about common data manipulation tasks. Well be looking at two cheatsheets as guides to the two main libraries we will use: ggplot and dplyr. These two are the yin and yang of data visualisation in R. With dplyr, well shape and squeeze our data into the right form to visualise what we want in ggplot. Were obviously not going to try and look at everything on these sheets. Well pick out a small number of examples to illustrate the underlying principles, with the hope that by the end of the day, you should have enough of an understanding of them to carry on your learning from them. Youve got copies of each of the cheat sheets. If you need them in future, theyre both available via RStudios help menu (or search for them online). The day is split roughly into two: Well learn some of the essential concepts of data wrangling in preparation for visualising by walking through a few typical script creation scenarios: getting our data, processing it, deciding what we want to do with it to prepare for visualising. For this part of the day, Ill be working through the material with you step by step, carrying out exactly the same tasks. Things will doubtless go wrong, often just misplaced brackets and the like - please ask if youre stuck. The second part will be more free-form. There are a range of choices for you to pick in this half of the day. Youll get to work through whichever you like, with help on hand (and youll get a break from me talking the whole time.) If theres anything else you want to have a go at, let me know and Ill try to help. Were going to try and fit a lot into one day: dont worry too much if not all of it makes sense as long as the overall picture you get does make sense. Ill try and make sure that everyone knows which bits they really need to have taken in. Well only scratch the surface of what R and the tidyverse can do. But it should be a solid foundation for further exploration. Well check in after every chunk of work to make sure everyones OK with where weve got to. Each part of the day builds on the other - its important that you feel like its making enough sense that you dont get lost as we move on. Please do ask if anything is confusing. Any new programming concepts, even for experienced programmers is always confusing. I learned Java before R: that didnt stop R being baffling for a long while. Our data for the day: Land registry data on house prices in England. (Well also look at some other data and link it.) All of the data for the workshop is open access: you can download it yourself for free. Ive provided links and notes at the end of this document for that. Our guiding question for digging into the data will be simple: can we see what impact the 2007/8 financial crash had on the English housing market? We wont be using any sophisticated methods - the plan is just to see what visualising the data might show us. Some random tips before we get started properly: Id recommend typing out as many of the R commands well be working with where you can in the first part of the day - its by far the best way to learn and understand the code. But if you get bored of that, or for some of the larger chunks of code later, feel free to use any of the code sources Ive supplied. The digital PDF and webpage also have links to the topics were covering, if you want to look at any of those during the day. Learning some keyboard shortcuts helps massively. You dont have to use them but, once learned, theyre hugely useful in R-Studio. Ill explain them as we go along. Ive also included a keyboard shortcut sheet for the ones you might want to use today. Oh and whats data wrangling? I refer you to Wikipedia. Really, it just means re-arranging data until its the shape we need. It has its own word now because we spend far more time doing this re-arranging than actual analysis. Right! &gt; "],["getting-started.html", "5 Getting started", " 5 Getting started OK, lets start by sorting out our project folder so we can access the data were all going to be working from. Well first need to copy the data folder to our own machines. Once youve done that, Id recommend making it a new RStudio project via the top right button: Navigate to your copied files and make a new project there. We can now access all the files and save our R script and visualisations here. Once youre done today, you can copy all this over to your USB stick or keep on your own drive. So now: create a new R script for your days programming and we can begin. Either use the menu (file/new) or the shortcut CTRL+SHIFT+N. "],["loading-the-libraries.html", "6 Loading the libraries", " 6 Loading the libraries Many of the most important tidyverse libraries are packaged into one - including ggplot and dplyr, the two well be using most heavily. Well start our script by loading this library. Stick this, and any later library loads, right at the top of your script: library(tidyverse) When it loads, youll see the many different libraries that come in the tidyverse displayed in the console. If the library isnt yet installed, youll get an error. Youll need to install it first. Because this only needs doing once, its best to do it in the console. If you discover any other libraries we use today are not yet installed, its same procedure. Note: when installing, the library name needs to be in quote marks. But its not in quote marks when we actually load the library. install.packages(&#39;tidyverse&#39;) "],["loading-the-house-price-data.html", "7 Loading the house price data", " 7 Loading the house price data OK, lets get stuck in to the data were going to use. You can use the file pane, bottom-right, to have a look at the files. Click on the data folder: as well as the little example dataframe we just looked at, this folder contains the following file: landRegistryPricePaidTopTTWAs.rds: a selection of Englands ten largest cities (by count of sales). It is in RDS format: these are compressed R objects - theyre much smaller files than CSVs would be. Lets load the data. Try a hugely useful autocomplete feature: when typing the file name, get data/land typed and then press CTRL + SPACE: this should list file names in the data directory. Were after this one: sales &lt;- readRDS(&#39;data/landRegistryPricePaidTopTTWAs.rds&#39;) The head function will show the top few rows  head(sales)  and will look something like this: Each row is a single property sale. As you can see, the data has eleven columns with the following variables: price: the final sale value of the property date: when it was sold, to the day. This field is in a format that R knows is a date. postcode: the location of the property being sold type: whether the property is a detached house (D), a semi-detached house (S), a terraced house (T) or a flat (F). Those four are all from the Land Registry data. Then we also have some extra location information: Eastings and Northings: these two columns give the exact geographical location for the postcode. (Its centroid.) wardcode: these are codes for the English ward the sale is in. There are many of these for each city and town. ttwa and ttwa_code: these are the travel to work area the sale is in. TTWAs are designed to capture each commuting area, so cities and towns will usually be at the centre of them. localauthority and localauthoritycode: well use this later to match to wage data and work out a consistent house price metric. Checking what time period the data covers with the range function, we can see weve got house sales from 1995 up until mid-June this year (2017) - weve got a bit more than two decades worth: range(sales$date) ## [1] &quot;1995-01-01 UTC&quot; &quot;2018-12-23 UTC&quot; TIP! If you need to read data in from a flat file like a comma-separated variable file, use the readr package from the tidyverse. Well be using this later. Base R reads csvs in with read.csv(). readr uses read_csv - using an underscore not a dot. Why use readr? It takes care of several otherwise annoying data format issues - most usefully for us, if you have dates in there, it makes them ready to be used straight away. Well get to know the geographies better as we visualise them. But first "],["lets-jump-right-into-ggplot.html", "8 Lets jump right into ggplot", " 8 Lets jump right into ggplot We can start making some ggplots immediately: these wont be pretty, but we can use them to illustrate the basics of how ggplot works, step by step. You should have ggplot already loaded as part of the tidyverse. All well do to start with: plot the locations of some of the sales, so we can see what the overall geographical reach of the data is. As you can see from the environment pane, there are several million sales in the data (one sale per row). That would take a long time to plot, so were just going to take a smaller sample using dplyrs sample_n function. This takes a random sample of rows from the dataframe. Well store the sample in a new variable. Here, were just getting ten thousand rows / sales. saleSample &lt;- sample_n(sales,10000) Lets plot our sample of sales using ggplot. Get this coded first, then well talk through what we just did. Another new thing to note here: the second line follows a plus at the end of the first. If you type the plus then press enter, RStudio will automatically indent the next line for you. ggplot is fussy about this plus - it needs to be at the end of the line, it cant be at the beginning of the next one: ggplot(saleSample, aes(x = Eastings, y = Northings)) + geom_point() So we can see house sales in different locations across England. But how did that work? Take a look at the cheatsheet: basics on page one has a little template explaining what ggplot requires to plot. It needs just the three things we supplied: The data. In this case, saleSample. That goes first. The mapping. This is done in the aesthetic function. (Thats what aes is short for.) In aes, we map our variables to the graphics aesthetics. The important principle here: each mapping requires a single column of data. This one requirement dictates how we must shape our data for ggplot. This is so important well go over it again in a moment! Here, weve mapped two variables to the x and y aesthetics (which wed usually think of as just a graphs x and y axes) - but there are bunch of others. More on that next. the geometry. Again, look at your cheat sheet, page 1. Almost all of the first page gives you all the different geometries ggplot can use. We used geom_point so that x and y are mapped to points. You can also get a quick guide to the geoms using RStudios autocomplete. This will list them all and give a little help overview. As with all other functions, for the full help page, use e.g.: ?geom_point While were here: try the zoom button and export button above the plot. Each of the options will give you a pop-out version of the plot that can be re-sized with the corner handle. It can then either be saved or copied to the clipboard. Well look at how to save the plot programmatically shortly. If you look at geom_point on the cheatsheet, youll see this list: alpha, colour, fill, shape, size, stroke. This is the list of other aesthetics you can map variables to when youre using the point geometry. This list differs depending on what you use - the cheatsheet tells you which you can use for each geometry type. These mappings all happen in the aes function, same as we did for x and y. So, for example, we can map colours to the TTWA / city. ggplot(saleSample, aes(x = Eastings, y = Northings, colour = ttwa)) + geom_point() This is not a good map! But it illustrates how aes works. There are a few other new things here. the ttwa variable is discrete: its a list of different places. ggplot figures this out and assigns one colour per category. Well look at what happens with continuous variables below. ggplot has also automatically added a legend. So back to the most important principle: ggplot wants each variable in its own column, in `long form. This applies to each variable you map to an aesthetic. Heres a little illustration to help with the intuition. If weve mapped ttwa to the colour aesthetic thenggplot will work out how to map a colour to each discrete group within that variable. If the variable had been continuous, ggplot would have supplied a continuous scale. Later, well look at what to do if your data isnt already in long form. You can of course choose what values to assign to x and y. And notice what happens if you assign a discrete variable to one of the axes. Here we put property sale price on y and ttwa on x: ggplot(saleSample, aes(x = ttwa, y = price)) + geom_point() Again - ggplot knows its discrete and labels each place. The plot itself is, again, fairly awful: overlapping points and labels, poor axis ticks etc. Well come back to all the prettifying things like labelling later, but lets stick to the essentials for now. So if we want to know something about the impact of the crash, well need to start using dates. But using them raw isnt a great idea: ggplot(saleSample, aes(x = date, y = price)) + geom_point() ggplot can plot dates correctly and provides axis ticks - but using each day is far too messy. We need to do two things to fix this: Create a better date category - well break the data down by year. Summarise the housing data based on this. There are two ways to summarise data for ggplot: ggplots own stat functions (more on these after the next section) Wrangling into the right shape ourselves using dplyr and other R functions. While rather more work, this is the only way to fully control what you want to visualise. Learning about wrangling and dplyr will be a big chunk of the day. First-up though: making a useable date column. "],["the-lubridate-library.html", "9 The lubridate library", " 9 The lubridate library Time for another library: lubridate. As youd expect, this makes working with dates a lot easier. Put this library call with the others at the top of the script. library(lubridate) Again, if its not already installed, do so in the console: install.packages(&#39;lubridate&#39;) The date column in the original data is already in datetime format. This is handy: formatting this ourselves is a faff. (See the tip above about using readr to make sure dates load in this format.) It means that we can now use the lubridate library to add a column containing the year of the property sale: sales$year &lt;- year(sales$date) If you go back to your dataframe view tab or use the console, youll see its been added: We can use the table function to get a quick look at the result, counting the number of sales per year: table(sales$year) You can also get sales per year by type: table(sales$year, sales$type) "],["using-ggplots-own-stat-functions.html", "10 Using ggplots own stat functions", " 10 Using ggplots own stat functions Now theres a year column, we can start summarising the data for each year. Going back to the original question - what impact did the 2007/8 crash have on the housing market - we can start by asking: What happened to the number of sales? To find this out, we can just count the number of sales per year. As mentioned, one way to do this is to use ggplots own summarising functions. These can be a really useful way to quickly get an overview of the data. For the plots done so far, all mappings have taken the variables value and mapped it directly to a value on the graph. Instead, ggplot stats create a summary variable first and then plot it. This makes more sense when seen in practice. Heres a simple bar chart: all its doing is counting the number of sales in each year: ggplot(sales, aes(x = year)) + geom_bar() So it looks like, for these TTWAs, the number of sales halved after the crash and never recovered. As before, we can assign the TTWA to the bar colour - though this time its fill (colour is the bar outline). London has vastly more sales per year than any other TTWA. ggplot(sales, aes(x = year, fill = ttwa)) + geom_bar() There are a few principles going on here: ggplot will use defaults where it can. In this case, weve not included a y-axis variable mapping in aes because the stat in geom_bar counts the number of observations in each year and so doesnt need it. Indeed, if you try and give it a y variable, it will throw an error: ggplot(sales, aes(x = year, y = price, fill = ttwa)) + geom_bar() Some geometries use ggplot stats, others will plot your data directly. How to tell the difference? Youll become familiar with the ones used for summary stats - on the cheatsheet, a lot of them are under one variable, discrete x, continuous y and continuous bivariate distribution. Well cover some more in a moment. But theres a principle at work: every geom has a stat. Its just that some have stat = identity, telling them to plot the data directly. Take a look at the help for geom_bar: ?geom_bar The stat argument is actually telling the geom to use a specific stat function: these are all listed in the left bar of page 2 of the cheatsheet. In the help file for geom_bar, its stat function is also there: stat_count. These can actually be used interchangeably. Note that stat_counts default geom is bar: ggplot(sales, aes(x = year, fill = ttwa)) + stat_count() Under usage, the help page lists all of geom_bars arguments: the ones given here are its defaults. These are the two most important: stat = count: tells geom_bar to count observations in each group position = stack: if there are groups, this tells geom_bar to stack them on top of each other. The cheatsheet lists these on page 2. We can override the default simply. For example, telling geom_bar to find the proportion of sales in each year by filling from top to bottom: ggplot(sales, aes(x = year, fill = ttwa)) + geom_bar(position = &#39;fill&#39;) In comparison, look again at the help for geom_point, that we used to begin with: ?geom_point Its default stat is identity: this mean it maps data points directly to the aesthetic. Another principle here: Weve said that ggplot will attempt to plot your data if it can, but certain geometries require certain types of data. As with geom_bar - it requires a categorical x variable, in this case weve given it each year. If wed wanted to plot a continuous x variable as a bar chart, theres one under one variable continuous on the cheat sheet. So say we wanted to use date directly, geom_histogram will put them into equal sized bins: ggplot(sales, aes(x = date)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. There are a lot of different stat functions, each with their own default geoms. When first learning ggplot, its just a matter of getting to know them through use. ggplots help is really helpful! Itll help for working out what default stats each of them uses. Well come back to one or two of these later. For now, on to using dplyr to shape data for visualising: "],["getting-started-with-dplyr-and-piping.html", "11 Getting started with dplyr and piping 11.1 Piping! 11.2 dplyrs verbs 11.3 And a few extras while were here", " 11 Getting started with dplyr and piping Were going to be mainly using dplyr to reshape our data into different forms for the visualisations. (Weve already loaded this library as part of the tidyverse package.) Before we get to dplyr itself - 11.1 Piping! Piping is magic: it will make everything spectacularly easier and tidier. Specifically, this is the pipe operator. It loads with dplyr and looks like this: A quick recap: pretty much everything in R is a function. For example, to find the square root of a number, use the sqrt function: sqrt(100) ## [1] 10 Functions can nest in functions. If, for some reason, you wanted to find the base 10 log of the square root of 100, you could nest like this: log10(sqrt(100)) ## [1] 1 And so on, if you had more complex things to accomplish. The pipe operator makes this much more intuitive and readable. So instead of the above, we can do this: 100 %&gt;% sqrt() ## [1] 10 100 %&gt;% sqrt() %&gt;% log10() ## [1] 1 So what were doing here: piping the result from one function into the next until we get the output were after. The number 100 is piped into the sqrt function - the output from that function (10) is piped into the log function. The power of this is: It lets us create a conveyor belt for what we do with data in a way that can very easily be changed at any stage in the process. dplyr is designed with this in mind. As with the assignment operator, typing percent-more-than-percent every time you want the pipe operator is a pain: so RStudio has another shortcut key. This time its: CTRL + SHIFT + M. Try this in the next bit of coding. This is also on your one page shortcut key guide. 11.2 dplyrs verbs We can use the pipe operator to link dplyr verbs together - these are functions that reshape the data in various useful ways. Well go through them one by one and end with a ggplot. Lets start by finding a summary statistic. Well find the average price in the housing data. Its good practice to have each dplyr verb on its own line. Notice how RStudio helps with this too: pressing return at the end of one line, after the pipe, automatically indents the next line. Well do the same with ggplot in the next section. sales %&gt;% summarise(mean(price)) ## # A tibble: 1 x 1 ## `mean(price)` ## &lt;dbl&gt; ## 1 225522. Verb 1: summarise. This passes all values from the column name you pass - in this case price - and we tell it to find the mean. It then gives you a single row back with the summary in. Notice here: dplyr (like almost all tidyverse functions) takes in bare variable names. It knows the variable is from sales as weve just piped that in. But thats the average price for the whole time range - what about one year? Verb 2: filter. We can filter by year. So weve now got two pipe operators: sales %&gt;% filter(year == 2018) %&gt;% summarise(mean(price)) ## # A tibble: 1 x 1 ## `mean(price)` ## &lt;dbl&gt; ## 1 390511. The filter verb uses logical operators to query each row in the data. The full list of these is on the data wrangling cheatsheet. Here they are again: Note especially the double equals we just used: this allows us to find exact matches for values and text (in this case, matching to 2015). A quick reminder of how R uses logical operators to create TRUE/FALSE vectors. (TRUE/FALSE values are also known as boolean values.) Heres an illustration of two logical operators checking their condition against each row of the chosen variable: So all filter does is take in a vector like this  ttwa==&#39;Bradford&#39; ## [1] FALSE FALSE FALSE TRUE TRUE TRUE TRUE FALSE FALSE FALSE  and then use that to keep the TRUE values. This is worth repeating, as once thats clear, you can produce any kind of boolean vector you like to filter the data. More on this shortly. So: filter picked out one particular year. But what if we want average price for each year? Verb 3: group_by. This will group the data by the category or categories we give it. The next function will then apply to each group. Youll get one row per group with the mean in: sales %&gt;% group_by(year) %&gt;% summarise(mean(price)) ## # A tibble: 24 x 2 ## year `mean(price)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1995 79425. ## 2 1996 84693. ## 3 1997 94675. ## 4 1998 104499. ## 5 1999 120137. ## 6 2000 137448. ## 7 2001 151563. ## 8 2002 173274. ## 9 2003 188621. ## 10 2004 212300. ## # ... with 14 more rows Thats given us average price per year - but has just showed some of them in the console. To be able to work with this returned dataframe, we need to assign it to a variable name: saleSummary &lt;- sales %&gt;% group_by(year) %&gt;% summarise(mean(price)) Youll see saleSummary has appeared in the environment pane: click on it to view the new summary data. One more thing before making another graph: if we want to find out average price per year and per city/ttwa, just include both in the group_by function. Well add one more thing here too. dplyr defaults the summary variable name to mean(price) - but we can set the name ourselves in summarise if we want something more sensible: saleSummary &lt;- sales %&gt;% group_by(ttwa,year) %&gt;% summarise(meanPrice = mean(price)) ## `summarise()` has grouped output by &#39;ttwa&#39;. You can override using the `.groups` argument. saleSummary now has one column each for ttwa and year in long format, each with a price average. Which is exactly what we need to map each of those to aesthetics in ggplot. Note also: an advantage of this separate-line approach to coding is: its easy to try different things, keep them and comment them out. So for instance, for the previous code, we could have kept the filter verb in place in case we wanted to come back to it: saleSummary &lt;- sales %&gt;% #filter(year == 2018) %&gt;% group_by(ttwa,year) %&gt;% summarise(meanPrice = mean(price)) This can now go straight into ggplot: ggplot(saleSummary, aes(x = year, y = meanPrice, colour = ttwa)) + geom_point() 11.3 And a few extras while were here There are a number of other dplyr verbs and well cover some of them below, but for now, lets just make some additions to what weve done, to introduce some new ideas. Using another geometry First: we can use a different geometry - we dont have to stick to points. With data over time, it makes sense to link time points with a line. As you might guess, the geom_line geometry will do this. Comment out geom_point for now and add it: ggplot(saleSummary, aes(x = year, y = meanPrice, colour = ttwa)) + geom_line() #geom_point() Layering geometries Youre not limited to one geometry: you can layer them. So if we just un-comment geom_point (and make sure theres a plus at the end of geom_line): ggplot(saleSummary, aes(x = year, y = meanPrice, colour = ttwa)) + geom_line() + geom_point() Note that our aesthetic mapping of colour to TTWA has applied to both lines and points. More on that shortly. What if we want geom_point to represent something different? Well Summarising for more than one variable Say we want to show the number of sales for each TTWA in each year. The first thing to do is count the sales via dplyrs summarise. This requires an update to our previous dplyr code - just add another summary variable to the summarise function: saleSummary &lt;- sales %&gt;% group_by(ttwa,year) %&gt;% summarise(meanPrice = mean(price), countOfSales = n()) ## `summarise()` has grouped output by &#39;ttwa&#39;. You can override using the `.groups` argument. Youll see we now have countOfSales as well as meanPrice: What does n() do? Its dplyr shorthand for number of observations: if weve grouped the data, it will give us the number of observations per group. (You can see this and other summarise functions on page 2 of the dplyr cheatsheet.) Now we have a count of sales, we can use this directly in geom_point. This is done by using the aes function in the geometry itself: ggplot(saleSummary, aes(x = year, y = meanPrice, colour = ttwa)) + geom_line() + geom_point(aes(size = countOfSales)) The size of the points now represents count of sales. ggplot gives it its own legend. Two things to note here: If youve mapped an aesthetic in the top ggplot function it will cascade to all other geometries. However, we can over-ride this within those geometries using their own aes function. The order of geometries determines which draws first. This can be used to control the look. Well do that next. The order of drawing and deciding where to map aesthetics Say we dont want colour to be mapped to both points and lines. There are a couple of options - one (the most flexible) is to overwrite the colour mapping in geom_point by choosing our own colour directly. We do this by setting the points colour outside of aes to what we want. Were not mapping to any variable this time, just setting it to a single value. There are a list of named colours we can use, including grey (more on this list below): ggplot(saleSummary, aes(x = year, y = meanPrice, colour = ttwa)) + geom_line() + geom_point(aes(size = countOfSales), colour = &#39;grey&#39;) The general principle here: Each list of aesthetics under the geometries on the cheatsheet can be either mapped to a variable (by including them in the aes function ) or set directly to a single type. Setting these within a geom has the effect of over-riding the cascading value. In this graph, points are drawn over the lines. This might look better if they were under them. To do this, just change the order. Having everything on separate lines makes this a little easier, though be careful to make sure pluses are in the right place. Also: remember you can use ALT + up/down arrows to move lines of code. So, shifting geom_point behind geom_line (and remembering to shift the plus as well): ggplot(saleSummary, aes(x = year, y = meanPrice, colour = ttwa)) + geom_point(aes(size = countOfSales), colour = &#39;grey&#39;) + geom_line() # &lt;&lt;&lt; moved this down one line and moved the plus too "],["using-scales-to-control-appearance.html", "12 Using scales to control appearance", " 12 Using scales to control appearance The last graph was getting there: its possible to see that prices were rising up until the crash - well need some more tweaks to properly compare places with different average house prices. (The count of sales isnt the clearest way to show that the rate dropped - our earlier geom_bar did that better, but it illustrates using two geoms.) It would be good to change a few things to make this more clear. One of the most essential ways of controlling appearance is through scales. The principle here is: Every aesthetic mapping has its own scale. Each of these can be controlled. This is true for the x and y mappings as well as everything else, including the colour, fill and size mappings weve used so far. The basics are: All scales are controlled using functions that begin with scale_. Page 2 of the ggplot cheatsheet lists them. As the cheatsheet shows, the format is (for example) **scale_*_continuous** - replacing the asterisk with the mapping we want to control. The most obvious kind of scale control youll want to do is on the x and y axes mappings. Particularly with prices, the most common of these has its own function: scale_y_log10 and scale_x_log10 both change the scales to log (base 10). For house prices, this is ideal: it makes proportional change comparable so that (e.g.) Londons much higher prices can be visually compared to cheaper TTWAs. This scale can be added with one line: ggplot(saleSummary, aes(x = year, y = meanPrice, colour = ttwa)) + geom_point(aes(size = countOfSales), colour = &#39;grey&#39;) + geom_line() + scale_y_log10() # &lt;&lt;&lt; new log y scale Where previously there appeared to be a marked difference in the post-crash response, the log scale shows that is perhaps not the case. London is still its own thing, as always. Note: these are 10 of the richest areas. There will be a chance later to compare richer and poorer TTWAs to see if there was any difference. Scale changes for other aesthetics have a range of options. An example will make this more clear. Say you want to control the size range for the points in the previous plot. Add the following scale code. ggplot(saleSummary, aes(x = year, y = meanPrice, colour = ttwa)) + geom_point(aes(size = countOfSales), colour = &#39;grey&#39;) + geom_line() + scale_y_log10() + scale_size_continuous(range = c(0,10)) Another note: for things like scale changes, it doesnt matter where in the ggplot argument order they go. Line order only makes a difference for the order that geoms draw in. geom_points circles are larger - but London is dominating, having by far the most sales. One option is to remove London to see what the others look like. We can use the dplyr verb filter to do this: its just a function, so as with anything else in R, it can be used anywhere. Here, we tell filter to give us all ttwas except London: ggplot(saleSummary %&gt;% filter(ttwa!=&#39;London&#39;), aes(x = year, y = meanPrice, colour = ttwa)) + geom_point(aes(size = countOfSales), colour = &#39;grey&#39;) + geom_line() + scale_y_log10() + scale_size_continuous(range = c(0,10)) Why did all the points become larger? Because the top of the range was previously London prices, making all the others relatively lower. Removing London changes that. Note, the range sizes appearance will vary depending on the size of the overall plot. This can be seen if you look at it via zoom or export/copy to clipboard. Is grey really working? If we want to try those points coloured by ttwa again, just remove the override (delete colour = grey): ggplot(saleSummary %&gt;% filter(ttwa!=&#39;London&#39;), aes(x = year, y = meanPrice, colour = ttwa)) + geom_point(aes(size = countOfSales)) + #&lt;&lt;&lt; removing colour = &#39;grey&#39; geom_line() + scale_y_log10() + scale_size_continuous(range = c(0,10)) Well, thats messy! One more option that can sometimes make otherwise unreadable graphs workable: we change change alpha. This controls transparency: alpha = 1 means perfectly opaque alpha = 0 means perfectly transparent So giving geom_point a value between those two (and note, were setting it directly so its outside of the aes function): ggplot(saleSummary %&gt;% filter(ttwa!=&#39;London&#39;), aes(x = year, y = meanPrice, colour = ttwa)) + geom_point(aes(size = countOfSales), alpha = 0.3) + #&lt;&lt;&lt; adding alpha value here geom_line() + scale_y_log10() + scale_size_continuous(range = c(0,10)) "],["scales-for-controlling-colour.html", "13 Scales for controlling colour", " 13 Scales for controlling colour Weve been using ggplots default colour scheme so far but - as with other scales - we can choose a colour scheme for the colour mapping ourselves. There are a huge number of options here, many on page 2 of the ggplot cheatsheet - lets just cover two. As before, we select the aesthetic by having colour in the middle of the function name (or fill if youre working with, for example, the colour fill of bars.) Well look at these two options: scale_colour_brewer scale_colour_manual The first of these gives us a range of pre-set palettes to choose from, based on the colour brewer palettes. Theyre broken into three types: sequential, qualitative and diverging. There is a function for viewing them (this is also on the ggplot cheatsheet) but you need to load the RColorBrewer package: library(RColorBrewer) display.brewer.all() To apply one of these to our data, we just need to add the following. Pick a palette name from the list then add this to the end of your previous ggplot code (remembering the plus on the previous line!) -&gt; scale_color_brewer(palette = &#39;Paired&#39;) ggplot(saleSummary %&gt;% filter(ttwa!=&#39;London&#39;), aes(x = year, y = meanPrice, colour = ttwa)) + geom_point(aes(size = countOfSales), alpha = 0.3) + #&lt;&lt;&lt; adding alpha value here geom_line() + scale_y_log10() + scale_size_continuous(range = c(0,10)) + scale_color_brewer(palette = &#39;Paired&#39;) Another option is to set the colours manually. There are a huge number of pre-defined colour names in ggplot. This website has the full reference: sape.inf.usi.ch/quick-reference/ggplot2/colour Or you can also use hex values. Heres some I took from www.color-hex.com. We have nine TTWAs to colour so Ive got nine values. Either set them directly in ggplot or put them in a vector to keep the code tidy. newcols = c(&#39;#ff9500&#39;,&#39;#154935&#39;,&#39;#f7786b&#39;,&#39;#410c37&#39;, &#39;#1242b6&#39;,&#39;#cc554c&#39;,&#39;#946b2d&#39;,&#39;#e0301e&#39;,&#39;#607d8b&#39;) Then use scale_colour_manual to apply them: scale_color_manual(values = newcols) ggplot(saleSummary %&gt;% filter(ttwa!=&#39;London&#39;), aes(x = year, y = meanPrice, colour = ttwa)) + geom_point(aes(size = countOfSales), alpha = 0.3) + #&lt;&lt;&lt; adding alpha value here geom_line() + scale_y_log10() + scale_size_continuous(range = c(0,10)) + scale_color_manual(values = newcols) "],["using-factors-to-control-order-of-variables.html", "14 Using factors to control order of variables", " 14 Using factors to control order of variables Up to now, the TTWAs have appeared in alphabetical order in the legend - which is a bit visually confusing, as the highest prices are nearer the top. It would be nice if the legend matched this. This is usually a pain to deal with in R but, again - tidyverse to the rescue. The forcats library helps us with categorical data like place names. Lets load it and see what it does: library(forcats) If you dont want to think about what its doing, you can just use forcats to re-arrange the order of TTWAs directly in ggplot. Building on our previous graph (though adding London back in), we can just use the fct_reorder function directly in ggplot. Give it the variable to reorder and another variable to reorder by, in this case meanPrice. Notice the minus sign: this will make it descending order. ggplot(saleSummary, aes(x = year, y = meanPrice, colour = fct_reorder(ttwa,-meanPrice))) + geom_point(aes(size = countOfSales), alpha = 0.3) + geom_line() + scale_y_log10() + scale_size_continuous(range = c(0,10)) Well look a bit more at what fct_reorder does when covering the dplyr mutate verb below. Theres a lot more about controlling graph appearance to talk about, but a last couple of tips for controlling graph view before moving on for now First-up: if you want to control the part of the data thats viewable, you can use one of the coordinate functions. With coord_cartesian, the exact window can be set. So say we only want to look at data between the years 2000 and 2008: ggplot(saleSummary, aes(x = year, y = meanPrice, colour = fct_reorder(ttwa,-meanPrice))) + geom_point(aes(size = countOfSales), alpha = 0.3) + geom_line() + scale_y_log10() + scale_size_continuous(range = c(0,10)) + coord_cartesian(xlim = c(2000,2008)) # &lt;&lt;&lt; set the view window Second-up: how to control the legends? If theyre in the wrong order, or you want to remove them entirely, use the guides function. If we want to force the count of sales size legend below the TTWA colour guide, set the order directly by adding this to the ggplot code: guides(colour = guide_legend(order = 1), size = guide_legend(order = 2)) Or to get rid of a legend entirely, just use its aesthetic name and set it to FALSE (or F for short - both work): guides(size = F) Therell be a lot more info on controlling appearance later. "],["joining-data-sources-together.html", "15 Joining data sources together 15.1 `Gathering into long form 15.2 Saving and reloading the housing data 15.3 Merging the housing and wages data 15.4 Choosing a subset based on some criterion", " 15 Joining data sources together It would be very useful to be able to adjust our house price data to something thats consistent over time. But how? We could use some value for inflation, but this doesnt actually include house price change and would only provide a single adjustment figure for the whole of England. Another option is to use wages over time. Not only does this give a simple measure of house cost - house price as a multiple of the average wage - but it varies by region, giving a much more localised measure. To do this, we need to link the housing data to wage data. Linking two datasets together is often an essential part of visualisation when there is a need to compare or supplement information. Rs base commands are not bad for this, but dplyrs join functions do the job a little bit quicker. So first, lets load the wage data (taken from NOMIS). Its in CSV format and well use the readr library mentioned earlier, loaded as part of the tidyverse package. Again, remember - you can type data/med and use autocomplete to get the rest of the file name. wages &lt;- read_csv(&#39;data/medianWages_localAuthority.csv&#39;) Click on the new dataframe in the environment panel: it consists of: a column each for the local authority name and code (the mnemonic column) One column per year from 1997 to 2018 15.1 `Gathering into long form We want to merge this with the housing data summary so that each median wage per year per local authority is matched against the average price at that time and place. But this is currently wide data - the year variable is spread across columns. So the first thing to do: gather the years into their own column so that it matches the single year column in the house sales data. This can be done with another tidyverse library: tidyr. This just requires two things: Give tidyr a name for the key and value columns. You can decide on these names. Lets go for a key of year and value column name as medianwage. Tell it which columns to gather. We can use the pipe operator. This looks like: wagesLong &lt;- wages %&gt;% gather(key = year, value = medianwage, `1997`:`2018`) Note: in this case, the column names in the range we passed in needed surrounding with backticks (very top left on the keyboard). Why? If column names are numbers, R cant parse them well - the tick marks are needed to pass in the names correctly. If they had been plain text, the ticks marks wouldnt be needed. Another joyous R foible. Take a look via the environment pane: we now have the wage data in long form. Note: we didnt have to say anything about the remaining local authority columns: tidyr automatically repeats these for us. Now its in long form, heres an opportunity to use another stat from ggplot ggplot(wagesLong, aes(x = year, y = medianwage)) + geom_boxplot() A boxplot can help get a quick look at the shape of the data. In this case, its easy to see median wages going up over time - so the data looks sensible. 15.2 Saving and reloading the housing data Because memory is a little tight, lets load a different selection of the housing data. This is in exactly the same format as before, its just sales from a selection of local authorities, rather than TTWAs. If you want to save your previous sales data or any of your other dataframes, now would be a good time. We can then overwrite some of these files without fear of losing anything. Although weve only added the year field so far, so nothing drastic will be lost. The saveRDS function is a good way to do this: it saves the dataframes as compressed objects that (a) are very compact and (b) reload in exactly the same form. So for instance, saving to our data folder: saveRDS(sales,&#39;data/salesTTWAwithYearAdded.rds&#39;) And feel free to save any other dataframes you want to return to. We can then overwrite the sales variable with the new price data. Again, its a long file name so use autocomplete again! sales &lt;- readRDS(&#39;data/landRegistryPricePaidLocalAuthoritySelection.rds&#39;) And well also need to re-add the year column: sales$year &lt;- year(sales$date) 15.3 Merging the housing and wages data Now weve got both of our datasets to combine, lets stop for a moment and think through what were aiming for. The (long) wage data gives us the median wage for each year and each local authority - one row for each of these combinations. We want to know how many multiples of this wage youd pay for the average-priced house in each local authority. Which means, first - we need to repeat our dplyr summarising, but this time at the local authority level, so it matches our wage data. We want it to also have one row per year / local authority combination. So we first need to summarise the sales data as we did before. Notice how dplyr makes this a simple change compared to before. We just replace ttwa with the new geography: saleSummary &lt;- sales %&gt;% group_by(localauthority,year) %&gt;% summarise(meanPrice = mean(price)) ## `summarise()` has grouped output by &#39;localauthority&#39;. You can override using the `.groups` argument. Looking at the result, we can see this matches the structure of wagesLong: one column for year and one for the local authority. This stuff is (for me at any rate!) always a little confusing!. It is rarely an intuitive process! Taking the time to look at the data in the View panel can be really helpful for making sense of whats going on. Were now ready to merge in the median wages. The dplyr cheatsheet has a neat diagram showing our various joining options. We want to use an inner join because we only want to keep local authorities that the two dataframes have in common. When doing joins, its generally wise to make a new variable. Joins often go wrong on first attempts: its useful to be able to compare with the previous data to make sure its done what you wanted it to. Note also: when merging/joining, its often necessary to do some checks that the data is formatted in the same way in both cases for the join fields, especially for variables like postcodes that may contain spaces. Well see an example of this in a moment The dplyr cheatsheet shows how to use join to merge on single variables - thats nice and straightforward. But there are two more things to consider here: We want to join on both year and local authority. This is solved by passing in a vector of the columns to join on. The variable name containing the local authorities is not the same in both dataframes. This is solved by supplying both names connected with an equals. This looks like this, and should work price_n_wage &lt;- inner_join( wagesLong, saleSummary, by = c(&#39;year&#39;, &#39;Area&#39; = &#39;localauthority&#39;) )  but, oh dear, not quite. What went wrong? This is a rare occasion when an R error message is actually informative: ``Cant join on year x year because of incompatible types (character / numeric) Ah: the year variable in one of them is in character format, not number. But which? Looking in the environment panel, we can click on the arrow symbol to look. This reveals, under wagesLong: The `chr next to year show its a character column. So when we gathered the year column names, they were kept as characters. Before we can merge, we have to make them the same format: wagesLong$year &lt;- as.numeric(wagesLong$year) Looking back in the environment panel, wagesLongs year column is now showing itself as num. And now we should be able to join: price_n_wage &lt;- inner_join( wagesLong, saleSummary, by = c(&#39;year&#39;, &#39;Area&#39; = &#39;localauthority&#39;) ) Check price_n_wage via the environment panel: weve now got mean price and median wage in the same rows. Now weve got those columns together, there are two more things to do: Its currently the weekly median wage. We want yearly, to get house price as a multiple of yearly pay. Once weve got yearly pay, turn the house prices into a multiple of this. How? Time for: Verb 4: mutate. This allows us to make whole new variables and attach them to our existing dataframe. All mutate does: it creates new variables and attaches them to the existing dataframe. This can be something as simple as a single value. Or, say we want to create a flag indicating areas with a median wage above £300 a week. Using the pipe operator, that would look like: price_n_wage &lt;- price_n_wage %&gt;% mutate(wages_above_300 = medianwage &gt; 300) Incidentally, we dont really need that column, so we can drop it with select and using a minus: price_n_wage &lt;- price_n_wage %&gt;% select(-wages_above_300) We want to use mutate for those two jobs: convert the weekly wage to a yearly one, and divide the mean property price by the yearly wage to get a wage multiple value as a house price index. As with summarise, mutate can do multiple variables at the same time. And, very handily, dplyr is also a lazyeval function. Which means what? This: We can define one variable and then use it immediately in the next one. So we can work out yearly pay, then immediately use that to find the house price multiple. Thus: price_n_wage &lt;- price_n_wage %&gt;% mutate( medianwageyearly = medianwage * 52, # weekly wage to yearly wage wagemultiple = meanPrice / medianwageyearly # house price as multiple of that yearly wage ) Look again the the price_n_wage dataframe to check its done what we wanted - and then we can try a plot. There are too many local authorities to fit in a legend, so we can use the guides code mentioned above when we plot, to remove the legend so we can get a look at all the data together without the legend messing up the plot. (Try it without the guides function to see what I mean.) ggplot(price_n_wage, aes(x = year, y = wagemultiple, colour = Area)) + geom_line() + guides(colour = F) So: we now have house prices as a ratio of median yearly wages. And theres one crazily high outlier and its difficult to see any effect from the crash. But there are a lot of local authorities here - how to pick out ones we want to look at? 15.4 Choosing a subset based on some criterion A useful first step to deciding how to pare down your data is to make an ordered list of some variable of interest to look at. So say we want to see: The order of wage multiples in the final full year of the data The advantage of making a separate list is: we can then use it as a tool for selection. But first, lets make it. We can just use filter to pick out the year we want - but heres a new verb: Verb 5: arrange. This re-orders the actual dataframe much as sort does in Excel. Thats quite different to anything else weve look at, where the actual row order of the data itself didnt really matter. (It has no impact on grouping or the order of category drawing, for instance). This is really useful for two reasons: first, if we actually want to see the order ourselves. Second, there are some functions that depend on data order, such as lag (see the window functions on the dplyr cheatsheet). So heres the code: use filter to get 2018 only then use arrange to sort the whole dataframe by the wage multiple. Make it in descending order with the minus. price_n_wage2018 &lt;- price_n_wage %&gt;% filter(year == 2018) %&gt;% arrange(-wagemultiple) Take a look at the new dataframe: the highest wage multiple is at the top and we can see what local authority it applies to. No surprise, London local authorities dominate the top of the list. And right at the top, way out in the lead, is Kensington and Chelsea. Could this be a combination of Englands most expensive housing with a mixed population (so a lower median wage)? We can use a dataframe like this to guide subsetting decisions for graphs. The essential principle is to use our selecting dataframe to subset the zones we want. Say we want to look only at the ten local authorities with the lowest wage multiple in 2018. By looking at the data, we can see this is those with a wage multiple below 5.78. zoneselection &lt;- price_n_wage2018 %&gt;% filter(wagemultiple &lt; 5.78) Or, because the data is ordered by wage multiple, we could just grab the bottom ten from the dataframe using the tail function (the opposite of head, this gives us the last rows): zoneselection &lt;- price_n_wage2018 %&gt;% tail(10) Once have this selection, we can tell ggplot: ``filter areas by those %in% our zone selection. This is the same principle as when we removed London - using a logical operator in filter: ggplot(price_n_wage %&gt;% filter(Area %in% zoneselection$Area), aes(x = year, y = wagemultiple, colour = Area)) + geom_line() The %in% operator is fantastic: it can be used for multiple selections. As with everything else in R, it works with vectors - we just passed it a dataframe column, which is a vector. If we wanted to pass a vector directly, we could - e.g.: ggplot(price_n_wage %&gt;% filter(Area %in% c(&#39;Camden&#39;,&#39;Oxford&#39;,&#39;Wirral&#39;,&#39;Liverpool&#39;)), aes(x = year, y = wagemultiple, colour = Area)) + geom_line() So the impact of the crash appears to be quite apparent in some of the local authorities with the cheapest housing. Wage multiples appear to have decreased over time since the crash. You might also want to select particular row numbers based on the data you can see in the View panel. Row numbers are shown on the left. Say we want to look at the top and bottom five but exclude Kensington and Chelsea. We can use a new dplyr verb: Verb 6: slice. Pass in the row numbers you want - simple. Put a minus before the selection to keep all but those. Place in a vector for multiple selections. #Top ten, excluding Kensington and Chelsea (no vector function needed) zoneselection &lt;- price_n_wage2018 %&gt;% slice(2:11) #Top and bottom five, excluding Kensington and Chelsea #Use a vector function for multiple selection zoneselection &lt;- price_n_wage2018 %&gt;% slice(c(2:6,72:76)) So: plotting our selected top and bottom five. Lets also add the fct_reorder code we used before to make the legend order more readable. And well relabel the legend while were at it. ggplot(price_n_wage %&gt;% filter(Area %in% zoneselection$Area), aes(x = year, y = wagemultiple, colour = fct_reorder(Area,-wagemultiple))) + geom_line() + labs(colour = &#39;area&#39;) The advantage of using a selection dataframe or vector like this: the ggplot code doesnt need to change. You just need to change the selection. Pausing for a moment: looking at the plot of top and bottom local authorities by wage multiple, its easier to see something about the impact of the crash: places with cheaper housing have seen a drop in value, relative to wages. Pricier areas have carried on their upward climb, after a brief dip. Well come back to the wage multiple data in the facets section later, if you want to look at this in more detail. Well stop there, but its worth mulling the following: if we plot wage multiple of house price versus the actual wage ggplot(price_n_wage2018, aes(x = wagemultiple, y = medianwage)) + geom_point()  while theres mostly a linear positive relationship, there are outliers in both directions: Kensington and Chelsea weve already seen, thats the point on the right. But there are also places with very low wage multiples but high median wages. How might you dig into that further? "],["saving-ggplots-as-image-files.html", "16 Saving ggplots as image files 16.1 Time for a pause!", " 16 Saving ggplots as image files Lastly, lets save a plot as an image file. To do this, we need to assign the plot to its own variable. If we re-run the previous ggplot of top and bottom wage multiples, its done like this - assigning to a variable as we would with any other assignment: zoneselection &lt;- price_n_wage2018 %&gt;% slice(c(2:6,72:76)) output &lt;- ggplot(price_n_wage %&gt;% filter(Area %in% zoneselection$Area), aes(x = year, y = wagemultiple, colour = fct_reorder(Area,-wagemultiple))) + geom_line() + labs(colour = &#39;area&#39;) If you run that, youll see the plot does not immmediately output to the plot window. To do this, just call the variable, as weve done with any other in the console. This will output our plot: output Now that weve got the plot assigned to output, we can use it to save as an image. If there isnt already a (currently empty) images folder in your project root folder, create one now. You can do that within RStudio using the New Folder option in the Files tab. Our output variable containing the plot goes into the ggsave function: ggsave(&#39;images/topAndBottomLAs_wageMultiples.png&#39;, output, dpi = 300, width = 7, height = 4) Some other points about ggsave: It works out what image format to save from the file name. In this case, it saves a PNG. DPI controls dots per inch. Higher values will be higher-resolution. Width and height will determine the relative size of text and features in the plot, not dpi. So, for example, see what happens when doubling both width and height: ggsave(&#39;images/topAndBottomLAs_wageMultiples_large.png&#39;, output, dpi = 300, width = 14, height = 8) Whereas if we make a low-res version but keep the dimensions the same: ggsave(&#39;images/topAndBottomLAs_wageMultiples_lowres.png&#39;, output, dpi = 75, width = 7, height = 4) 16.1 Time for a pause! So youve successfully loaded data, wrangled it and visualised it. Just a quick reflection on how we did that, before moving on to the second part of the day: We used a bunch of tidyverse libraries to process and visualise data. These are all designed with the same philosophy of `constrain to simplify. ggplot works with long data: a single column maps to a single aesthetic. For any particular feature, we fed it a single variable in a dataframe column. If that column contained a bunch of categories that we wanted to give a colour to, we just told ggplot: colour = variable. Same for any other aesthetic, including x and y coordinates. OK, so next - youve got six (and a bit) options to have a go at. For this part of the day Im going to let you work independently from this booklet and be on hand to help. If anything important comes up, Ill work through it on the screen again. Working on prettifying the plot youve just made. This covers some of the most common options for customising plots. Facetting and dodging: facetting is a way to use ggplots aesthetics to create multiple plots in one. Dodging is a draw-position function that allows side-by-side comparison of a variable. This section puts both together and looks at house type versus wage multiple. Then we have two options for working on iteration: Outputting multiple plots. Rather than having to manually output individual plots, this introduces a programmatic way to loop through all the data you want to output, giving each its own plot and saving them all to a folder. Pulling out model values and using them to plot, including error rates. (And a quick look at functional programming.) ggplot has specific geoms for plotting ranges and error rates. This section looks at pulling coefficients and standard errors from a series of models into one dataframe, so they can all be plotted. Theres also a look at creating your own functions as a way of repeating the same similar tasks. (This is the and a bit) And lastly a little exercise using dplyr to summarise some geographical data and find a groups modal value: Mapping! This section introduces two ways to make maps from your data: first, using ggplot itself and, second, using the simple features package - an amazing spatial analysis tool that plays nicely with the tidyverse. Freestyle! Working from the cheatsheets, try and find another way to play with and visualise the data. Or repeat what weve already done with some different data. In order to aid this process, if you want to try it, theres a different set of data in the data folder to the one we originally used. Rather than a selection of largest TTWAs (in terms of their number of sales) this one contains ten of the largest and smallest, some of which are TTWAs with much lower house prices. So this is a chance to ask: is there a difference in how the richest and poorest areas in England (in terms of house price) responded to the crash? Weve already seen some evidence of a difference - and theres more on this in option 4. The file is called: landRegistryPricePaidTopBottomTTWAs.rds And this one doesnt have its own section below: youre on your own! Work with the code above and the cheatsheets. A start would be just swapping the RDS file in the first section and seeing how different it looks. "],["six-and-a-bit-options.html", "17 SIX (and a bit) OPTIONS", " 17 SIX (and a bit) OPTIONS "],["prettifying-a-graph.html", "18 Prettifying a graph", " 18 Prettifying a graph So we made some graphs, but theyre lacking finesse. ggplot provides many ways to customise its appearance to make it more presentable. In this option, well look at some of the most common functions used to do this. So heres the plot we were just working on, for reference - but feel free to go and grab code for the earlier plots if youd prefer. This will all apply to those too. Here well look at the top and bottom local authorities wage multiple (excluding Kensington and Chelsea, as we did before). Note: this section uses the same price_n_wage dataframe from the section on merging. If youve done the facet option before this one, you may have over-written it. If so, heres a quick option to reload. (Or run the code for that section again, whichever you prefer.) #reload price_n_wage data we previously made, if it&#39;s been over-written with anything price_n_wage &lt;- readRDS(&#39;data/price_n_wage_fromjoinsection.rds&#39;) price_n_wage2018 &lt;- price_n_wage %&gt;% filter(year == 2018) %&gt;% arrange(-wagemultiple) #choose whichever zone selection option you want. Here&#39;s the previous three. #zoneselection &lt;- price_n_wage2018 %&gt;% filter(wagemultiple &lt; 5.78) #zoneselection &lt;- price_n_wage2018 %&gt;% slice(2:11) #We&#39;ll use this one here. #Top and bottom five, excluding Kensington and Chelsea #Use a vector function for multiple selection zoneselection &lt;- price_n_wage2018 %&gt;% slice(c(2:6,72:76)) Weve already seen how to change the label for the legends using labs and control legend order using fact_reorder - these are included here: ggplot(price_n_wage %&gt;% filter(Area %in% zoneselection$Area), aes(x = year, y = wagemultiple, colour = fct_reorder(Area,-wagemultiple))) + geom_line() + labs(colour = &#39;area&#39;) The basic principle is to add cumulatively to a ggplot: just add a plus at the end of a line and then include our new tweak on its own line. An advantage of it being on its own line: we can comment out any particular feature easily to play around with the output. Try any / all of the following. If you want to see them all together, skip ahead a few pages. Change the axis labels The x and y axis labels can be changed with the xlab and ylab functions. So updating the y axis would look like: ylab(&#39;wage multiple&#39;) Add some better ticks to the y axis scale You can set any scale ticks completely manually. This can be done by adding a scale_y_continuous function. scale_y_continuous(breaks = c(0,5,10,15,20,25,30)) Its also possible to over-ride the labels used for those ticks. Be careful to make sure both of these vectors are same length - it needs a one-to-one correspondence: scale_y_continuous(breaks = c(0,5,10,15,20,25,30), labels = c(&#39;0&#39;,&#39;5x&#39;,&#39;10x&#39;,&#39;15x&#39;,&#39;20x&#39;,&#39;25x&#39;,&#39;30x&#39;)) This tells ggplot exactly which breaks we want and how to label them. (The labels can be text or numbers.) So we have very fine control of these. Giving the plot a title I always forget how to do this and need to google it! I invariably copy the code from here and amend it. We add a ggtitle and then make it bold by changing the theme. The latest ggplot aligns titles to the left by default - you can include an hjust value in theme ggtitle(&quot;House prices as multiple of yearly wage top and bottom five English local authorities&quot;) + theme(plot.title = element_text(face=&quot;bold&quot;,hjust = 0.5)) Thats a stupidly long title. You can wrap lines in the title by using backslash n where the lines should break. Its hard to spot here: the backslash n is after wage and before top: ggtitle(&quot;House prices as multiple of yearly wage\\ntop and bottom five English local authorities&quot;) Removing axis titles entirely One might think having the year title for the x axis is a bit superfluous: years are self-explanatory. (Many say axes must always be labelled, mind!) Anyway, if you dont like it, the theme function can again be used. Note here, were adding the to theme formatting we did for title above. theme(plot.title = element_text(face=&quot;bold&quot;,hjust = 0.5)), axis.title.x=element_blank()) The same can be done with text and ticks using axis.text.x and axis.ticks.x (or y). Changing the line type and size As we did when changing the sales count circles to grey, the line type and size can be altered in the geom_line function directly. Heres a reference for the linetype number: 0 = blank, 1 = solid, 2 = dashed, 3 = dotted, 4 = dotdash, 5 = longdash, 6 = twodash geom_line(size = 1, linetype = 5) Quickly creating a flag for a new aesthetic mapping Perhaps you want to mark out which local authorities are in London. All we need for this: a single column marking which of the local authorities are in London. We could then use linetype to mark this. In the current data, the three London zones are Camden, Islington and Hackney. You could obviously use some more sophisticated way to pick out your group but lets just make it directly: londonzones &lt;- c(&#39;Camden&#39;,&#39;Islington&#39;,&#39;Hackney&#39;) We can then use dplyr to make a flag column (with actual labels rather than just a zero and one, as is common with flag columns.) Theres a new function here: ifelse. As the name suggests, this just lets us say: if this condition is met, do this - else, do that. (Thats the best way to remember the order of the arguments too: `if, then: this, else that.) We can combine it with the %in% operator. price_n_wage &lt;- price_n_wage %&gt;% mutate(inLondon = ifelse(Area %in% londonzones, &#39;london&#39;,&#39;other&#39;)) As always, take a look at the dataframe to confirm its done what you wanted it to. Or throw it into the ggplot and see Bringing all that together Heres those options all in the ggplot code - including the new linetype aesthetic mapping. It also hides the inLondon variable name above the linetype legend. ggplot(price_n_wage %&gt;% filter(Area %in% zoneselection$Area), aes(x = year, y = wagemultiple, colour = fct_reorder(Area,-wagemultiple))) + geom_line(aes(linetype = inLondon), size = 1) + ylab(&#39;wage multiple&#39;) + labs(colour = &#39;local authority&#39;, linetype = &#39; &#39;) + scale_y_continuous(breaks = c(0,5,10,15,20,25,30), labels = c(&#39;0&#39;,&#39;5x&#39;,&#39;10x&#39;,&#39;15x&#39;,&#39;20x&#39;,&#39;25x&#39;,&#39;30x&#39;)) + ggtitle(&quot;House prices as multiple of yearly wage\\ntop and bottom five English local authorities&quot;) + theme(plot.title = element_text(face=&quot;bold&quot;,hjust = 0.5), axis.title.x=element_blank()) "],["facetting-and-dodging-getting-as-much-info-into-one-graph-as-humanly-possible.html", "19 Facetting and dodging: getting as much info into one graph as humanly possible", " 19 Facetting and dodging: getting as much info into one graph as humanly possible Theres a quote about visualisation doing the rounds: ``A designer knows he has achieved perfection not when there is nothing left to add, but when there is nothing left to take away. Thats all very well, but sometimes its good to stuff as much information into one chart as possible This section covers two useful things to help us do this: ggplot has a rather nice feature called facetting that allows us to put data into separate panels in one plot. As with other mapping of aesthetics, to do this we need one column that will act as our facet category, to split across multiple panels. The position function in geometries also has a very useful option: dodge. Weve already seen position = stack: this stacked mapped variables on top of each other in a bar chart so their counts summed. Instead, dodge places them next to each other. Well also use this as an opportunity to explain a bit more how factors work. Lets ask: How have prices (in terms of wage multiple) of different types of house changed (detached, terraced etc) for a range of years, in four local authorities? Again, this is the wage data combined with local authority house price data. In case this isnt already loaded, heres the code again. Note, we already know the year column will need converting to numeric, so we do that here too. wages &lt;- read_csv(&#39;data/medianWages_localAuthority.csv&#39;) #Make wages into long data, year in its own column wagesLong &lt;- wages %&gt;% gather(key = year, value = medianwage, `1997`:`2018`) #Make year numeric in preparation for the join wagesLong$year &lt;- as.numeric(wagesLong$year) #Reload sales at local authority level sales &lt;- readRDS(&#39;data/landRegistryPricePaidLocalAuthoritySelection.rds&#39;) #Add the year column sales$year &lt;- year(sales$date) As before, we need to make a summary of the house price data - but with an addition. We want average price per year, per place AND per house type. As always, you can just add this to the group_by verb and dplyr will create the grouped data already in long form. We also initially filter to get just four specific years, two each side of the crash. Each of these will get its own facet. saleSummary &lt;- sales %&gt;% filter(year %in% c(2000,2005,2010,2015)) %&gt;% group_by(localauthority,year,type) %&gt;% summarise(meanPrice = mean(price)) ## `summarise()` has grouped output by &#39;localauthority&#39;, &#39;year&#39;. You can override using the `.groups` argument. Were now ready to join, as we did before. We use inner_join again: so this will only keep years (and local authorities) common to both, in this case just 2000, 2005, 2010 and 2015. Because the saleSumary data now has type in long form, the join will repeat values for the joined wages dataframe. Take a look once joined, you can see there are repeated wages for each house type (but unique price averages). Lets also add the yearly wage and wage multiple columns here. Note: you could of course join these two separate tasks with the pipe operator. It doesnt always help with readability though - sometimes its better to keep things a little separate. #See the dataframe: median wage repeated for each year / place price_n_wage &lt;- inner_join( wagesLong, saleSummary, by = c(&#39;year&#39;, &#39;Area&#39; = &#39;localauthority&#39;) ) #Make new columns price_n_wage &lt;- price_n_wage %&gt;% mutate( medianwageyearly = medianwage * 52, # weekly wage to yearly wage wagemultiple = meanPrice / medianwageyearly # house price as multiple of that yearly wage ) OK, the datas ready for plotting. A reminder of the principle here: ggplot wants one column per variable mapping / aesthetic. We do that by using long data. So now weve got each of the following in its own column: wagemultiple Area (the local authority) type (the house type) year The only new type of mapping we now do is the facet itself. Heres another R/ggplot quirk: unlike everything else, this one requires a tilde before the variable name. (Look up facet_grid to see why this is actually useful.) So lets try this: ggplot(price_n_wage %&gt;% filter(Area %in% c(&#39;Camden&#39;,&#39;Oxford&#39;,&#39;Wirral&#39;,&#39;Liverpool&#39;)), aes(x = Area, y = wagemultiple, fill = type)) + geom_col() + facet_wrap(~year) Hmm. Not quite: the wage multiples are stacked (this is geom_cols default position - check its help file to confirm), which doesnt make sense here. They should be side-by-side. Dodge to the rescue! Over-ride geom_cols default position (remembering the plus for the next line): geom_col(position = &#39;dodge&#39;) Ah ha! Each value is now side-by-side and makes sense against the y axis. But it would be useful to tell ggplot to place all the facets on one row to make it easier to compare the y-axis between categories. How? Use autocomplete to get your list of facet options. Put a comma after year then press CTRL+SPACE. Use the mouse or cursor: the help tells you how to use ncol, nrow and scales: So we can set all the facets on one row with: facet_wrap(~year, nrow = 1) Starting to look better, but the order and labels for the house type are poor. The order of the local authorities could do with tweaking too. For this, we need to return to factors and using forcats. Recall that ggplot uses factor order for ordering its legends and plots. (Or alphabetical / numerical order if the variables are plain character or number.) Weve already used fct_reorder: this orders a variable by another - weve used it to order place names by average house price. But it also converts the variable to a factor in the process as only factors can have a defined order in R. To see whats going on, its useful to look closer at the variables. The current type variable is a plain character: class(price_n_wage$type) ## [1] &quot;character&quot; The first thing well do is use forcats to recode the names to something more readable. Well make a new variable, type2, so we can compare the difference: in practice wed usually overwrite the original. Using mutate to carry out the recode, we make a new type2 variable from the old type: price_n_wage &lt;- price_n_wage %&gt;% mutate(type2 = fct_recode(type, &#39;flat&#39; = &#39;F&#39;, &#39;terrace&#39; = &#39;T&#39;, &#39;semi&#39; = &#39;S&#39;, &#39;detached&#39; = &#39;D&#39; )) As with fct_reorder, the act of recoding converts the variable to a factor. And we can see this factor has an order to it as well - this defaults to alphabetical. By converting to numeric, you can also see what factor does: the coding has a number under the surface, marking what order the factors are in. class(price_n_wage$type2) ## [1] &quot;factor&quot; unique(price_n_wage$type2) ## [1] detached flat semi terrace ## Levels: detached flat semi terrace as.numeric(unique(price_n_wage$type2)) ## [1] 1 2 3 4 A more sensible order for house type would be by size and likely cost. Use fct_relevel to do this (and this time over-write type2): price_n_wage &lt;- price_n_wage %&gt;% mutate(type2 = fct_relevel(type2, &#39;detached&#39;, &#39;semi&#39;, &#39;terrace&#39;, &#39;flat&#39; )) If you now repeat the previous look at the variable, you can see the levels have the order weve given them (as has the number order.) unique(price_n_wage$type2) ## [1] detached flat semi terrace ## Levels: detached semi terrace flat as.numeric(unique(price_n_wage$type2)) ## [1] 1 4 2 3 Its worth emphasising: this order has nothing to do with row order in the dataframe. Its the coding for the variable itself. OK, so thats enough about factors. Well just do one last thing: reorder place names by the wage multiple, as weve done before. This factor can overwrite the previous plain-character variable: price_n_wage &lt;- price_n_wage %&gt;% mutate(Area = fct_reorder(Area,-wagemultiple)) Bringing all that together, the plot should now look something like this. Note also that geom_col has its colour set directly. This makes the bar outlines a little better defined: geom_col(position = &#39;dodge&#39;, colour = &#39;grey&#39;) You could also try coord-flip - this can work well with categorical x axes. And - because were using three different categorical variables, try different combinations of mapping them to fill, facet and x axis. What works best? For example, if we swap year with Area and set a free y scale, we can look at changes within local authorities over time. Though you need to be careful to note that theyre less visually comparable, if a scale is free, ggplot will add individual tick marks for each. facet_wrap has a range of other options. Also useful is scales. As the link above explains (and as we saw in the autocomplete) this can be scales = free. Or also free_y, free_x to control them separately. In this case, free_y would allow ggplot to adjust each separately to fill the graph (but at the cost of the wage multiple not being as comparable.) "],["iteration.html", "20 Iteration 20.1 1. Outputting multiple plots 20.2 2. Pulling out multiple model values and visualising them", " 20 Iteration 20.1 1. Outputting multiple plots Its often useful to be able to look at many different elements of your data in their own separate plots - for example, looking at each city/town or local authority separately. This isnt often the kind of task you might need for presenting data - but as a way to understand your own data, being able to output many plots easily is really useful. One option is to use a for-loop to do this. This will allow us to loop over each plot we want and output them separately. What well do here: cut up the local authorities from the work done in the joining-data section into equal size groups and plot each of the groups. We can work with the list of local authorities in price_n_wage2018. As usual, the tidyverse supplies a feature for cutting numeric data up in various ways. If you havent still got price_n_wage and price_n_wage2018 in memory from the join section, heres the price_n_wage dataframe with code for re-grabbing just 2018. #reload price_n_wage data we previously made #if it&#39;s been over-written with anything or removed price_n_wage &lt;- readRDS(&#39;data/price_n_wage_fromjoinsection.rds&#39;) price_n_wage2018 &lt;- price_n_wage %&gt;% filter(year == 2018) %&gt;% arrange(-wagemultiple) So how to cut up the local authority data into equal size groups? Heres code that does that in two forms: the first is base R, the second uses the pipe operator in dplyr - just to compare legibility between the two. Theres very useful auto-help for the cut functions: if you just type cut_, this should appear. Each of the three cut functions is explained. cut_number will create a column marking out which rows for wagemultiple are in each group: #Base R version price_n_wage2018$groupToPrint &lt;- as.numeric(cut_number(price_n_wage2018$wagemultiple,8)) #dplyr piping version, same result price_n_wage2018 &lt;- price_n_wage2018 %&gt;% mutate(groupToPrint = wagemultiple %&gt;% cut_number(8) %&gt;% as.numeric ) Note: the cut functions are not cutting according to row - its done according to value. Thats why group number one is lowest-value local authorities and eight the highest. Get a list of the unique values were going to iterate over. This could be a vector of character names like place-names - it wouldnt have to be numeric. groupsToPrint &lt;- unique(price_n_wage2018$groupToPrint) groupsToPrint ## [1] 8 7 6 5 4 3 2 1 Now were going to loop over those eight groups and produce a plot for each. If youve not come across for-loops before, theyre straightforward. Translated to English, for-loops are just: for each of this set of values (in our case, itll be our eight groups), carry out this bunch of tasks, once per value. That vector can now be used in the for loop. Set up the for loop first just to see what its doing: for(grp in groupsToPrint){ print(grp) } The for-loop assigns each value from groupsToPrint to the grp variable in turn. It then executes the code between the curly braces. In this case, were just printing the grp number - but the principle is the same for whatever code we put between them. We just need to replace print(grp) with our code. The first job for the loop: pull out the list of local authorities to print on each iteration. We filter by group and then pull out the single vector of local authorities in the Area variable (we also use print here so the result is visible in the for loop): for(grp in groupsToPrint){ #get vector of zones to draw zoneselection &lt;- price_n_wage2018 %&gt;% filter(groupToPrint == grp) %&gt;% pull(Area) print(zoneselection) } A couple of things before starting: Well add our plots to a sub-folder in the images folder. Make that now with RStudios new folder option in the file tab, inside the images folder: something like groupsOfLocalAuthorities. Its tricky trying to debug code thats running inside a for loop. A useful thing to do is: pick one value to assign to grp so that we can then run the code by highlighting, without having to run the whole for loop. For example: grp &lt;- 1 When we run the actual for loop, it will overwrite this with the value it assigns. But we can work with it while getting the code right. Theres nothing much new in the code below: this is just our previous wage multiple plot code, working on each group in turn. The only other thing to note: At the bottom, were making a filename that includes the first and last local authority names in the group (as well as the group name). This is done with the paste0 function. Paste0 just takes in a bunch of bits of text and variables, separated by commas, and turns them into one character. This allows us to save each group as its own file. There is also an added geom_point showing the median wage itself. for(grp in groupsToPrint){ print(paste0(&#39;outputting group &#39;,grp)) #get vector of zones to draw zoneselection &lt;- price_n_wage2018 %&gt;% filter(groupToPrint == grp) %&gt;% pull(Area) output &lt;- ggplot(price_n_wage %&gt;% filter(Area %in% zoneselection), aes(x = year, y = wagemultiple, colour = fct_reorder(Area,-wagemultiple))) + geom_point(aes(size = medianwage), alpha = 0.2) + scale_size_continuous(range = c(0,10)) + geom_line(size = 0.75) + labs(colour = &#39;area&#39;) #save the plot #text of zones zonetext &lt;- paste0(zoneselection[1],&#39;_to_&#39;,zoneselection[length(zoneselection)]) filename &lt;- paste0(&#39;images/groupsOfLocalAuthorities/group_&#39;,grp,&#39;_&#39;,zonetext,&#39;.png&#39;) ggsave(filename, output, dpi = 300, width = 9, height = 5) } 20.2 2. Pulling out multiple model values and visualising them This section is inspired by code from the excellent book R for Data Science and the section on mapping with the purrr library (another one that comes packaged with the tidyverse library). Academic papers are still full of endless regression tables. They are not very aesthetically pleasing. Another option is to visualise your models findings in some way. This section gives an example - it has little statistical merit but illustrates the basic idea: If youre running any kind of model on multiple subsets of data, how can you pull out the results of interest and visualise them? How do you also show error? ggplot has a number of geometries specifically for showing ranges - as well as showing error, these can be used for e.g. showing mins and maxes. You must set the values for these yourself The basic principle here is simple: pull out all of the results you want into a dataframe, then apply your dplyr and ggplot knowledge to visualise it and create error values Wed expect employment and house prices to have a positive relationship but whats the magnitude of the relationship, how much does it vary between places and has it changed over time? To look at this, well combine house price data with employment data so that we have these in their own columns: Average house price per ward for a range of TTWAs, for 2001 and 2011 Percent of employed people in those wards for 2001 and 2011 Well look at the same house price data, but this version is subsetted to: 2001 and 2011 so we can compare to employment data from the Censuses in those years. Only TTWAs that have wards with 30 or more sales in each time period. If you like, theres the option of skipping ahead a few steps and just loading the pre-prepared data for the regressions. Otherwise, lets start by loading another subset of the sales data: sales &lt;- readRDS(&#39;data/sales2001and2011wardsWithMoreThan30sales.rds&#39;) #Note it&#39;s already got year in names(sales) ## [1] &quot;price&quot; &quot;date&quot; &quot;postcode&quot; ## [4] &quot;type&quot; &quot;Eastings&quot; &quot;Northings&quot; ## [7] &quot;wardcode&quot; &quot;ttwa&quot; &quot;ttwa_code&quot; ## [10] &quot;localauthority&quot; &quot;localauthoritycode&quot; &quot;year&quot; And this is the employment data: the columns 2001 and 2011 contain the percent of economically active people in employment in that ward. employment &lt;- read_csv(&#39;data/percentEmployedByWard2001n2011.csv&#39;) Now, as before, the job is just to: summarise the housing data: average price per ward and per year - with the added detail that we want to keep the TTWA name these wards are in. Join this summary with the employment data, linking on year and ward. Notice the trick used in summarise here to keep the TTWA name in the summary: using max(ttwa). What does this do? In short: its a way of easily grabbing a column we want in our summary, if we know theres a single value per group. (The sales dataframe has already had its wards set to only be in one TTWA.) #Average price per ward per year priceSummary &lt;- sales %&gt;% group_by(year,wardcode) %&gt;% summarise(meanprice = mean(price), count =n(), ttwa =max(ttwa)) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the `.groups` argument. #p.s. if we wanted to find the modal ttwa, we could use this. #ttwa = names(which.max(table(ttwa)) As the employment data has one column for each year, this has to be made into long-form. The two can then be joined using inner_join again to keep only wards common to both: empLong &lt;- employment %&gt;% gather(key = year, value = percentEmployed, `2001`:`2011`) %&gt;% mutate(year = as.numeric(year)) avpriceplusemp &lt;- inner_join(priceSummary,empLong, by = c(&#39;year&#39;,&#39;wardcode&#39;)) #count of wards per ttwa / year table(avpriceplusemp$ttwa, avpriceplusemp$year) ## ## 2001 2011 ## Blackburn 50 50 ## Bradford 32 32 ## Brighton 54 54 ## Cambridge 67 67 ## Doncaster 21 21 ## Hull 61 61 ## London 926 926 ## Middlesbrough &amp; Stockton 71 71 ## Oxford 66 66 ## Poole 24 24 Heres the pre-prepared and joined price and employment data, if you skipped the last chunk: avpriceplusemp &lt;- readRDS(&#39;data/avpriceplusemp.rds&#39;) So: the aim is to do a regression of house price on employment for each TTWA, using its wards as data points. Before attempting multiple regressions, though, lets work through a single regression to see what needs to happen. Well be using log of house price as the dependent variable and using this to tell us about percentage change. And well even use Rs base plotting function to quickly look at it. #test a single regression, look at plot. #Pull out a single year and TTWA testinz &lt;- avpriceplusemp %&gt;% filter(year == 2001, ttwa == &#39;Bradford&#39;) #run the regression and look at a summary - log of house price as rez &lt;- lm(data = testinz, formula = log(meanprice) ~ percentEmployed) summary(rez) ## ## Call: ## lm(formula = log(meanprice) ~ percentEmployed, data = testinz) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.27890 -0.16621 0.02439 0.13770 0.31521 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.804860 0.760030 6.322 5.69e-07 *** ## percentEmployed 0.066004 0.008179 8.070 5.22e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1921 on 30 degrees of freedom ## Multiple R-squared: 0.6846, Adjusted R-squared: 0.6741 ## F-statistic: 65.13 on 1 and 30 DF, p-value: 5.223e-09 #Look at the data and the estimated regression line plot(log(testinz$meanprice) ~ testinz$percentEmployed) abline(rez) In Bradford in 2001, an employment increase of 1% was associated with a house price increase of about 6.6%. But how to get the coefficients out of the ugly table and into a lovely graph? Youll have to work this out for whatever model type you use, but for lm it looks like this. Read the comments #Get a model summary object rezsummary &lt;- summary(rez) #Some things like R-squared are easily accessible... rezsummary$r.squared ## [1] 0.6846464 #But we want the slope coefficient and standard error. #Coefficients are stored in a matrix... rezsummary$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.80485956 0.760030304 6.321932 5.690004e-07 ## percentEmployed 0.06600402 0.008178535 8.070397 5.223239e-09 #... so we just reference using row and column position to get them: rezsummary$coefficients[2,1] # coefficient for employment ## [1] 0.06600402 rezsummary$coefficients[2,2] # standard error ## [1] 0.008178535 Now we know where to find the coefficients and standard errors all that needs to happen is: find a way to repeatedly run the model for each TTWA. Weve already used a for loop to iterate over a process. R has a few other methods for iterating over groups of objects - and again the tidyverse provides a really neat version: the map function from the purrr library. Since weve already used summarise, this is a good way to think about how map works. The notation is lengthy to explain - check the R for data science book if you want to know more. But this allows us to pull out the price means from each TTWA into a variable (of `double format, i.e. can have good decimal precision). Note: split is a base-R function that splits a dataframe into a bunch of dataframes, based on the category passed in. So this is splitting the sales dataframe into smaller ones for each TTWA. The purrr library should be already loaded as part of the tidyverse library. x &lt;- sales %&gt;% split(.$ttwa) %&gt;% map_dbl(~mean(.$price)) x ## Blackburn Bradford Brighton ## 74787.08 78759.26 186405.66 ## Cambridge Doncaster Hull ## 189188.19 76228.50 85228.74 ## London Middlesbrough &amp; Stockton Oxford ## 277824.28 89690.92 208710.30 ## Poole ## 189531.80 The notation used makes it very easy to create tidy iterations. And here, we can break down the process of running many models and pulling out the numbers of interest using exactly the same approach we just used to get those means. First, running a linear model on each TTWA for 2001. Note we combine map with the dplyr filter verb. models &lt;- avpriceplusemp %&gt;% filter(year == 2001) %&gt;% split(.$ttwa) %&gt;% map(~lm(log(meanprice) ~ percentEmployed, data = .)) And to check we can now use those models to access the coefficients in the way we previously worked out: models %&gt;% map(summary) %&gt;% map_dbl(~.$coefficients[2,1]) ## Blackburn Bradford Brighton ## 0.11050658 0.06600402 0.02570804 ## Cambridge Doncaster Hull ## 0.09532273 0.12828774 0.08965422 ## London Middlesbrough &amp; Stockton Oxford ## 0.03491091 0.07675510 0.06060024 ## Poole ## 0.10562224 Tick! Great. Now what? Well, we can use this repeatedly to directly create a dataframe. So using the check code we just tested and two others, the following code does this: Creates a dataframe of coefficients and standard errors (and pulls out the TTWA names in the process) Uses mutate to multiply the coefficients by 100 to give percentages Then uses mutate again to create a 95% confidence interval using the standard error alloutput &lt;- data.frame( names = models %&gt;% map(summary) %&gt;% map_dbl(~.$coefficients[2,1]) %&gt;% names, coeffs = models %&gt;% map(summary) %&gt;% map_dbl(~.$coefficients[2,1]), se = models %&gt;% map(summary) %&gt;% map_dbl(~.$coefficients[2,2]) ) #Multiply by 100 alloutput &lt;- alloutput %&gt;% mutate( coeffs = coeffs * 100, se = se * 100 ) #add in some min/max confidence intervals from standard error alloutput &lt;- alloutput %&gt;% mutate( minn = coeffs - (se * 1.96), maxx = coeffs + (se * 1.96) ) As always, take a look at the resulting dataframe. Weve got everything we need to plot. But notice the new thing here: geom_errorbar takes in values for ymin and ymax from the data: these geoms in ggplot are not doing any of their own stats - you need to supply the mins and maxes directly, which is why we calculated them. (This gives us a lot more flexibility about what we want to show.) ggplot(alloutput, aes(x = fct_reorder(names,-coeffs), y = coeffs)) + geom_point(size = 5) + geom_errorbar(aes(ymin = minn, ymax = maxx), width = 0.5) + coord_flip() As a last exercise: how would you go about running the models for both decades and then comparing? The aim, as with all ggplots, would be to have year in its own column so it can be mapped to an aesthetic. You could also use dodge. You could just re-run the code above, changing the year in filter to 2011. But another option is to turn the above code into a function. Doing this provides another way of iterating your code. Some points on functions: As weve already said, R is a functional language: everything is a function, and they can all be composed together in whatever way produces valid output. Functions are just input/output machines: you just need to tell it what goes in and what you want out. A basic function might look like this. This is obviously a trivial example but the idea applies to anything more complex - the structure of writing one is exactly the same. Say we want to know: is a number divisible by two? (As if this isnt obvious, but this illustrates the principle!) isDivisibleByTwo &lt;- function(x){ #Use R&#39;s modulus operator %% - gives the remainder from dividing by a number result &lt;- x %% 2 output &lt;- ifelse(result == 0, TRUE, FALSE) #We could also do this as the remainder from 2 is only 1 or 0 #and R interprets 1 and 0 as TRUE and FALSE #return(!as.logical(x)) #By default, an R function will return the last created variable. #Or you can tell it explicitly what to return like this. return(output) } You then need to highlight the whole code block and run it, as usual. Nothing will happen - except youll now see the function name in the environment panel. Once thats done, your new in-out machine should work: isDivisibleByTwo(46) ## [1] TRUE isDivisibleByTwo(127) ## [1] FALSE The principle of building functions is simple: where possible, put what doesnt change into a function, pass what does change as arguments to the function. In this simple case, its our number we want dividing by two that changes. (Note: the name of the variable is arbitrary: whatever you pass in will be assigned to it.) So: how would you apply this to the model-running code? Here are some ideas. The only thing that changes is the year: its either 2001 or 2011. Everything else can be the same as the code used above. All of this code could go inside the function: The first models &lt;- avpriceplusemp code, where currently year is set to either 2001 or 2011. Its here wed use the value passed into the function. Then the code creating the alloutput dataframe. This is the thing we want the function to return. There would also be nothing wrong with adding the extra year column in the function itself - but you would need that for creating a single year column that ggplot can use to map to an aesthetic. Note: you can view a working version of this function in the file containing all the code, in the project folder, if you want to see how its put together to help write your own / cheat! #Pass a year into the function, then add a column to mark the year. model01 &lt;- modelz(2001) %&gt;% mutate(year = 2001) model11 &lt;- modelz(2011) %&gt;% mutate(year = 2011) #Both will be the same structure - combine the rows from both bothmodels &lt;- rbind(model01,model11) And with the addition of position = dodge in geom_errorbar, this could be used to create the following. Note here were setting dodge spacing directly so we can control the point and error bar position to both match each other: Again, the R for Data Science book has a lot more on functional programming - hopefully this gives a hint of its power. "],["a-little-bit-of-mapping.html", "21 A little bit of mapping 21.1 One simple way to use ggplot for mapping: the 2D geoms 21.2 Using cowplot to get around the facetting problem", " 21 A little bit of mapping In this section, well take a quick look at three different ways to make maps from the housing data: The first two use ggplot again, to show two very simple ways it can be used to view geographical data. ggplot can do a lot more, but this is a start. Then well introduce a couple of new libraries: the simple features (sf) library and tmap for making maps. The sf library is relatively new: its particularly great because it works excellently with all the *dplyr data wrangling skills youve already learned. Well run through an example of doing that. 21.1 One simple way to use ggplot for mapping: the 2D geoms A really simple way to make quick maps is to use ggplots 2D geoms. One of these included on the ggplot cheatsheet - geom_bin2d - does a single job: as the help says, if you have two dimensions of continuous data, it: divides the plane into rectangles, counts the number of cases in each rectangle, and then (by default) maps the number of cases to the rectangles fill. We have exact postcode locations for each sale: these can be our two dimensions. So in practice that means we can easily make a map using this geom that counts how many sales there are in each rectangle. First, lets re-load the sales data (adding the year column back in) and subset to London: sales &lt;- readRDS(&#39;data/landRegistryPricePaidTopTTWAs.rds&#39;) sales$year &lt;- year(sales$date) london &lt;- sales %&gt;% filter(ttwa==&#39;London&#39;) Then try the new geom, subsetting to a single year: ggplot(london %&gt;% filter(year == 2018), aes(x = Eastings, y = Northings)) + geom_bin2d() A map! It could with a few little extras. The next version shows that you can: Control the number of bins by defining their dimensions. This geographical data is in British National Grid projection: Easting and Northing units are in metres. So we can make a map with 1km square grid by setting bin width to 1000. As with geom_bar and other bin stats, this is set in the geom itself. Pick a colour scale, as weve done before - but with one difference: use distiller. This is for continuous variables: it distills continuous values from color brewers discrete scales. (You can also add trans = log into this function if you want a log fill scale showing the correct values.) coord_fixed does what the name suggests: the x and y axis are fixed relative to each other. Its also possible to set a ratio for the two of them - check the help file. theme_void() drops almost all extras from the graph, just leaving the data itself and the guide/legend. ggplot(london %&gt;% filter(year == 2018), aes(x = Eastings, y = Northings)) + geom_bin2d(binwidth = c(1000,1000)) + scale_fill_distiller(palette = &#39;Spectral&#39;) + coord_fixed() + theme_void() But what about things other than count? What if we want to, say, find the median sale value per square? ggplot has a stat for that - though its not listed on the cheatsheet: stat_summary_2d. This gives us total control over what goes into each square. The only extra thing we need to do here: Provide a z value: this will be the column used in the grid square statistic. So well use price. stat_summary_2ds default is to give you the mean, but we can provide any function we like. So heres the median (and were including all the other extras we added to the last one). Note, weve only changed three things: the geom used, and adding z = price into the aesthetic. Weve also done as mentioned above: told the fill scale we want a log transformation: ggplot(london %&gt;% filter(year == 2018), aes(x = Eastings, y = Northings, z = price)) + stat_summary_2d(fun = median, binwidth = c(1000,1000)) + scale_fill_distiller(palette = &#39;Spectral&#39;, trans = &#39;log&#39;) + coord_fixed() + theme_void() Try finding the minimum and maximum property value per square (using the min and max functions). This reveals something interesting about the difference in property types in the centre versus the outskirts. You have a couple of options now: you can skip ahead to the next section if you want to do some proper mapping with actual map data, get to use the simple features package and learn how to wrangle spatial data. Or the last part of this section just shows quickly how to use the map function from the purrr library (also used in the pulling out multiple values section) to get around a problem with using facet (see the facet section). 21.2 Using cowplot to get around the facetting problem As the section above on facetting shows, ggplot can be used to facet produce many sub-plots based on a factor, like property type. However, facets all keep the same colour scale. This can be a problem if e.g. the count of flats and detached houses is very different: each individual map will have its scale compressed and look flat and boring. One workaround: create a number of individual plots and then combine them. We can do this with the cowplot library. As always, use install.packages if this isnt already installed. library(cowplot) This library can be used to very neatly arrange multiple ggplots. So lets aim to make a single plot showing the count of each different property type. We can start with the same code we just wrote for getting a count of properties per grid square, except subset to single property types: flats &lt;- london %&gt;% filter(type == &#39;F&#39;) flatplot &lt;- ggplot(flats %&gt;% filter(year == 2018), aes(x = Eastings, y = Northings)) + geom_bin2d(binwidth = c(1000,1000)) + scale_fill_distiller(palette = &#39;Spectral&#39;) + coord_fixed() + theme_void() flatplot terraces &lt;- london %&gt;% filter(type == &#39;T&#39;) terraceplot &lt;- ggplot(terraces %&gt;% filter(year == 2018), aes(x = Eastings, y = Northings)) + geom_bin2d(binwidth = c(1000,1000)) + scale_fill_distiller(palette = &#39;Spectral&#39;) + coord_fixed() + theme_void() terraceplot We can see how cowplot works now just with these two. We can combine multiple plots just by supplying them to plot_grid: plot_grid(flatplot,terraceplot) But how to avoid having to manually create each plot? There are several options, but heres a neat one using purrr. All this does: Uses base Rs split function to split the dataframe into each house type Passes each subgroup to the map function. Note, this is exactly the same ggplot code we just used (except were filtering to 2018 in the first line). The only other difference: weve replaced the dataframe name with the dot operator. The dot stands in for each of the smaller dataframes that map passes in. Note also the tilde (~): thats just shorthand for passing into a function. plots &lt;- london %&gt;% filter(year == 2018) %&gt;% split(.$type) %&gt;% map( ~ggplot(., aes(x = Eastings, y = Northings)) + geom_bin2d(binwidth = c(1000,1000)) + scale_fill_distiller(palette = &#39;Spectral&#39;) + coord_fixed() + theme_void() ) All thats done is map each of the plots we were previously doing manually to a list. We could view any of them by looking at the list directly. For example, heres the first: plots[[1]] Now that whole list can just be passed directly into cowplots plot_grid. (Were also now telling cowplot the number of rows and columns we want): plot_grid(plotlist = plots, ncol = 2, nrow = 2) OK, looking good. There are a couple of things it would be nice to do now. See if you can work out how. Some hints: We need to label each house type. Using info from the prettifying section, can you use ggtitle to add that in? Hint: use the max trick, also used in the pulling out multiple model values section to get a single value from the type column. There will only be e.g. a single F repeated in the flats sub-dataframe, so you can pull that out be e.g. doing this. You just need to use the dot operator instead. max(flats$type) Coordinates for each plot are currently different, because ggplot adjusts each to its min and max range. We could do with setting them to be the same. Hint: in the prettifying section, we used coord_cartesian and set the plots xlim values. We can set them here directly in coord_fixed. The limits were after are just the minimum and maximum for both eastings and northings. These could be dropped directly into the right place in coord_fixed range(london$Eastings) range(london$Northings) The result should look something like this: "],["making-a-map-using-the-simple-features-library-tmap-and-a-bit-of-wrangling.html", "22 Making a map using the simple features library, tmap and a bit of wrangling", " 22 Making a map using the simple features library, tmap and a bit of wrangling Another way to map data is the simple features library. As always, if its not installed, do so with: install.packages(&#39;sf&#39;) And then load it: library(sf) The sf library has been designed from the ground up to work with the tidyverse - so any kind of wrangling youve already learned can be used here too. If you want to learn more, take a look at the excellent free online book Geocomputation with R by Robin Lovelace, Jakub Nowosad and Jannes Muenchow. Well just be making a map here, but sf, combined with the tidyverse, is a powerful spatial analysis and manipulation tool. Well start by loading some map data - London wards. All well then do is make a map of the average house price per ward. #Note it appears as a standard dataframe. But look, geometries! londonwards &lt;- st_read(&#39;data/mapdata/londonwards.shp&#39;) ## Reading layer `londonwards&#39; from data source `C:\\Users\\admin\\Dropbox\\Training\\R_PrinciplesOfViz_and_Datawrangling_2021\\data\\mapdata\\londonwards.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 962 features and 1 field ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 501183 ymin: 149641 xmax: 576444 ymax: 208030 ## Projected CRS: Transverse_Mercator Take a quick look at the loaded spatial data. You can just use plot(londonwards) but that gives one map per column by default. To just check its looking correct, you can use: plot(st_geometry(londonwards)) Lets also reload the house sales data just for those London wards: #Use London-only ward subset: sales &lt;- readRDS(&#39;data/landRegistryPricePaid_LondonWards.rds&#39;) #Add year column back in sales$year &lt;- year(sales$date) The plan is to link these two - its always a good idea to make sure the link columns are behaving and there are no bad links. A good way to do this is just to table up a question: are the london wardcodes present in the sales wardcodes? #Check there&#39;s a good link between the mapping and housing data. Tick. table(londonwards$wardcode %in% sales$wardcode) ## ## TRUE ## 962 All true - so yes, its fine, well be able to successfully link the mapping and housing data on that column. Now, well just do as weve previously done: find a summary of the housing data per geographical zone, ward in this case. And filter down to a single year: #Summarise salesSummary &lt;- sales %&gt;% filter(year==2018) %&gt;% group_by(wardcode) %&gt;% summarise(meanprice = (mean(price)/1000) %&gt;% as.integer(), count = n()) So now that gives us single wards in each row (check by looking via the environment panel), each one summarised with a mean and count of sale number. We can now link this summary to our mapping data using the same dplyr join functions weve already used. But note, theres little wrinkle here. Lets run this twice, but with the dataframes in a different order: london1 &lt;- inner_join(salesSummary, londonwards, by = &#39;wardcode&#39;) london2 &lt;- inner_join(londonwards, salesSummary, by = &#39;wardcode&#39;) class(london1) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; class(london2)#Only this one is spatial ## [1] &quot;sf&quot; &quot;data.frame&quot; Both dataframes look the same if viewed but, if we check the class, only the second has sf as its first class. The lesson: if youre joining spatial sf data and want to keep the spatial part, put it first in a join. There are ways around that but this is the easiest way. Were now ready to make a map. This will require a new library: tmap. As always, install first if its not already installed, and then load: library(tmap) And then we can jump straight into making a map. All we do is supply the column value we want to plot in tm_polygons: tm_shape(london2) + tm_polygons(&quot;meanprice&quot;) A map! But it needs a few extras. First-up, you may want to pick a better colour scheme. As with color brewer, theres a function for looking at all the options: tmaptools::palette_explorer() There are a few other tweaks added here as well - the tmap help files explain the vast number of others available to you. That includes a great getting started page. Here, we make these additions: Set the style to jenks: this makes for a better visual spread of values. Set the number of legend categories with n to get a nicer spread. Set the palette, having picked one from the palette explorer. (Just put a minus before the palette name to reverse its order.) Reduce the border alpha so it doesnt block the polygons as much. (See the prettifying section for more on alpha values: 0 is totally transparent; 1 is opaque.) Move the legend outside of the main map box. This is a quick way to make sure legends dont clash with the map. See the ?tm_layout help for a load more legend positioning options. tm &lt;- tm_shape(london2) + tm_polygons(&quot;meanprice&quot;, style = &#39;jenks&#39;, n = 10, palette = &#39;viridis&#39;, border.alpha = 0.3) + tm_layout( legend.outside = T, legend.position = c(0.05,&#39;center&#39;) ) tm And because weve assigned the map to a variable, we can also save it: tmap_save(tm, filename = &quot;images/londonhouseprices_ward2018.png&quot;) "],["a-last-little-ggplot-mapping-example.html", "23 A last little ggplot mapping example", " 23 A last little ggplot mapping example Weve already seen at the start, if you have point coordinates you can map these to the x and y axes to get an idea of the geography of your data. Here, we just combine this idea with dplyrs summary ability to ask: Whats the most common type of house in each postcode in London? To do this, well apply a simple function that finds the modal property type. The general principle here is: you can use any function in summarise. So its the same sales data, containing the London TTWA. And we reduce it to London: sales &lt;- readRDS(&#39;data/landRegistryPricePaidTopTTWAs.rds&#39;) london &lt;- sales %&gt;% filter(ttwa == &#39;London&#39;) Then its good old dplyr again for finding the mode. Note we can use max here as the data has been grouped by postcode - we only have one location value per postcode, so the max is correct. cityModalTypes &lt;- london %&gt;% group_by(postcode) %&gt;% summarise(mode = names(which.max(table(type))), Eastings = max(Eastings), Northings = max(Northings)) To see how the mode is found, take some time to break down the full list of functions and check the help files too: table(city$type) which.max(table(city$type)) names(which.max(table(city$type))) As in the facetting section, we can also recode the property type to something more sensible: cityModalTypes &lt;- cityModalTypes %&gt;% mutate(mode2 = fct_recode(mode, &#39;flat&#39; = &#39;F&#39;, &#39;terrace&#39; = &#39;T&#39;, &#39;semi&#39; = &#39;S&#39;, &#39;detached&#39; = &#39;D&#39; )) %&gt;% mutate(mode2 = fct_relevel(mode2, &#39;flat&#39;, &#39;terrace&#39;, &#39;semi&#39;, &#39;detached&#39; )) And then plot! A couple of extras here to notice: We set the shape by number: the ggplot cheatsheet has a little guide to the available shape codes. the guides function is used to make the categories more clear - otherwise it defaults to the displayed size which, in this case, would be tiny. labs is used to remove the legend title - its clear enough from the labels what it refers to. Now it just needs some less hideous colours ggplot(cityModalTypes,aes(x = Eastings,y = Northings, colour = mode2)) + geom_point(shape = 15, size = 0.25) + scale_color_brewer(palette = &#39;Set1&#39;, direction = -1) + #https://stackoverflow.com/questions/20415963/how-to-increase-the-size-of-points-in-legend-of-ggplot2 guides(colour = guide_legend(override.aes = list(size=10))) + coord_fixed() + labs(colour = &#39;&#39;) + theme_void() "],["data-sources.html", "24 Data sources", " 24 Data sources The original data used in the workshop can be downloaded here: Land registry price paid data: www.gov.uk/government/statistical-data-sets/price-paid-data-downloads Code-point open postcode data containing geolocations for each postcode - scroll down the list here: www.ordnancesurvey.co.uk/opendatadownload/products.html Employment / economically active data from the English 2001 and 2011 Census. My version is a bespoke geographically harmonised dataset - the original data is available at CASWEB. If anyone is interested in the harmonised dataset, get in touch. Wage data for English local authorities: NOMIS official labour market statistics Ward map data from borders.ukdataservice.ac.uk "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
